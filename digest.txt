Directory structure:
└── AI_Diplomacy/
    ├── lm_game.py
    ├── test_lm_game.py
    ├── test_stage0.py
    ├── test_stage1.py
    └── test_stage2.py

================================================
File: lm_game.py
================================================
import argparse
import asyncio
import logging
import os
import sys # For sys.exit
import time # For basic timing if needed outside of specific logs
import traceback # For exception logging
from typing import Optional # Removed Set

import dotenv
from diplomacy import Game
# Removed: GLOBAL, Message from diplomacy.engine.message (not used directly in new main)
# Removed: to_saved_game_format (handled by GameResultsProcessor)
# Removed: llm (direct use of llm library is now in coordinator)
# Removed: defaultdict, concurrent.futures (orchestrator uses asyncio.gather)

# New refactored components
from ai_diplomacy.game_config import GameConfig
from ai_diplomacy.logging_setup import setup_logging
from ai_diplomacy.agent_manager import AgentManager
# PhaseSummaryGenerator is used by GamePhaseOrchestrator, not directly in main
# from ai_diplomacy.phase_summary import PhaseSummaryGenerator 
from ai_diplomacy.game_orchestrator import GamePhaseOrchestrator
from ai_diplomacy.game_results import GameResultsProcessor
from ai_diplomacy.game_history import GameHistory
# DiplomacyAgent and AgentLLMInterface are primarily managed by AgentManager
# from ai_diplomacy.agent import DiplomacyAgent 
# from ai_diplomacy.llm_interface import AgentLLMInterface

# get_valid_orders is kept from the original utils.py / lm_game.py for now.
# gather_possible_orders might still be needed by get_valid_orders or other utils.
from ai_diplomacy.utils import get_valid_orders # Removed gather_possible_orders

# Removed old direct imports of assign_models_to_powers, conduct_negotiations, planning_phase,
# initialize_agent_state_ext, narrative related imports as these functionalities
# are now part of the new classes or deprecated in this simplified entry point.

# Suppress Gemini/PaLM gRPC warnings if still relevant (can be moved to logging_setup or utils if general)
os.environ["GRPC_PYTHON_LOG_LEVEL"] = "40"
os.environ["GRPC_VERBOSITY"] = "ERROR"
os.environ["ABSL_MIN_LOG_LEVEL"] = "2"
os.environ["GRPC_POLL_STRATEGY"] = "poll"

dotenv.load_dotenv()

# Logger will be configured by setup_logging
logger = logging.getLogger(__name__)

def parse_arguments() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Run a Diplomacy game simulation with configurable parameters."
    )
    # Arguments from original lm_game.py relevant to GameConfig
    parser.add_argument(
        "--power_name", type=str, default=None, 
        help="Name of the primary power to control (e.g., FRANCE). Optional."
    )
    parser.add_argument(
        "--model_id", type=str, default=None,
        help="Model ID for the primary power's LLM (e.g., ollama/llama3, gpt-4o). Optional."
    )
    parser.add_argument(
        "--num_players", type=int, default=7, 
        help="Number of LLM-controlled players. Default: 7."
    )
    parser.add_argument(
        "--game_id_prefix", type=str, default="diplomacy_game",
        help="Prefix for the game ID if not explicitly set. Default: 'diplomacy_game'."
    )
    parser.add_argument(
        "--game_id", type=str, default=None,
        help="Specific game ID to use. If None, one will be generated. Default: None."
    )
    parser.add_argument(
        "--log_level", type=str, default="INFO", choices=["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"],
        help="Logging level. Default: INFO."
    )
    parser.add_argument(
        "--log_to_file", type=lambda x: (str(x).lower() == 'true'), default=True,
        help="Enable/disable file logging. Default: True."
    )
    parser.add_argument(
        "--log_dir", type=str, default=None, # GameConfig handles default base if this is None
        help="Base directory for logs. Game-specific subfolder will be created here. Default: './logs'."
    )
    parser.add_argument(
        "--perform_planning_phase", action="store_true", default=False,
        help="Enable the planning phase for each power. Default: False."
    )
    parser.add_argument(
        "--num_negotiation_rounds", type=int, default=3, # Original default was 0, updated to common use
        help="Number of negotiation rounds per movement phase. Default: 3."
    )
    parser.add_argument(
        "--negotiation_style", type=str, default="simultaneous", choices=["simultaneous", "round-robin"],
        help="Style of negotiation rounds. Default: 'simultaneous'."
    )
    parser.add_argument(
        "--fixed_models", type=str, default=None,
        help="Comma-separated list of model IDs to assign to powers (e.g., 'gpt-4o,ollama/llama3'). Cycles if fewer than num_players."
    )
    parser.add_argument(
        "--randomize_fixed_models", action="store_true", default=False,
        help="Randomize assignment of fixed_models to powers. Default: False."
    )
    parser.add_argument(
        "--exclude_powers", type=str, default=None,
        help="Comma-separated list of powers to exclude from LLM control (e.g., 'TURKEY,RUSSIA')."
    )
    parser.add_argument(
        "--max_years", type=int, default=None, # Original was 1901, now optional
        help="Maximum game year to simulate. Game ends after this year's Winter phase. Default: No limit."
    )
    # --output argument from original is implicitly handled by log_dir/game_id structure in GameConfig for results.
    # If a specific single output file for game JSON is still needed, it can be added.
    # For now, results are saved in results_dir within the game_id_specific_log_dir.
    
    # Argument from original args.models, now more clearly named fixed_models
    # Acknowledging --models was for full power assignment, fixed_models is slightly different.
    # If the exact old --models behavior is needed, GameConfig/AgentManager logic might need adjustment.
    # For now, assuming --fixed_models covers the main use case for specifying non-primary models.

    return parser.parse_args()

# get_valid_orders is kept from original utils.py / lm_game.py for now.
# Its own LLM logic is not being refactored in *this* specific step.
# It will be passed to the GamePhaseOrchestrator.
# Signature: async def get_valid_orders(current_game, model_id, agent_system_prompt, board_state, power_name, possible_orders, game_history, model_error_stats, agent_goals, agent_relationships, agent_private_diary_str, log_file_path, phase)
# Note: model_error_stats is removed from its call signature as it's no longer passed down from here.
# get_valid_orders will need to be adapted if it still expects it, or if error stats are handled differently.
# For now, we assume get_valid_orders will be adapted or the parameter can be defaulted to None.

async def main():
    args = parse_arguments()
    
    # 1. Initialize GameConfig
    # GameConfig now handles deriving log paths, game_id etc.
    # It also converts args.fixed_models and args.exclude_powers from CSV strings to lists.
    if args.fixed_models:
        # Remove spaces after commas for robustness
        args.fixed_models = [m.strip() for m in args.fixed_models.replace(' ', '').split(',')]
    if args.exclude_powers:
        args.exclude_powers = [p.strip().upper() for p in args.exclude_powers.split(',')]
        
    config = GameConfig(args)

    # 2. Setup Logging (uses GameConfig)
    setup_logging(config) # Configures root logger, console, and optional file handler

    logger.info(f"Starting Diplomacy game: {config.game_id}")
    logger.info(f"Full configuration: {vars(config.args)}") # Log all parsed args
    start_time = time.time()

    game: Optional[Game] = None # Initialize game to None for finally block
    game_history: Optional[GameHistory] = None # Initialize for finally block
    agent_manager: Optional[AgentManager] = None # Initialize for finally block

    try:
        # 3. Create Game Instance
        game = Game()
        game_history = GameHistory()
        # Store game instance in config for access by other components if needed (orchestrator does this)

        # 4. Initialize AgentManager (uses GameConfig)
        agent_manager = AgentManager(config)

        # 5. Assign models and Initialize Agents
        # Use all powers from the game instance for assignment.
        all_game_powers = list(game.powers.keys())
        powers_and_models_map = agent_manager.assign_models(all_game_powers)
        
        if not powers_and_models_map:
            logger.error("No LLM-controlled powers were assigned. Exiting.")
            sys.exit(1) # Exit if no agents to run
            
        agent_manager.initialize_agents(powers_and_models_map)
        
        if not agent_manager.agents:
            logger.error("Failed to initialize any agents. Exiting.")
            # This might happen if all agent creations failed.
            sys.exit(1)

        # 6. Initialize GamePhaseOrchestrator
        # PhaseSummaryGenerator is now created on-the-fly by the orchestrator.
        orchestrator = GamePhaseOrchestrator(
            game_config=config, # type: ignore
            agent_manager=agent_manager,
            # Removed phase_summary_generator=None, as it's no longer an __init__ param
            get_valid_orders_func=get_valid_orders # Pass the existing function
        )

        # 7. Run Game Loop
        await orchestrator.run_game_loop(game, game_history)

    except KeyboardInterrupt:
        logger.info("Game interrupted by user (KeyboardInterrupt). Saving partial results...")
    except Exception as e:
        logger.error(f"An unexpected error occurred during the game: {e}", exc_info=True)
        # Ensure traceback is logged if not already by default logger settings
        detailed_error = traceback.format_exc()
        logger.error(f"Detailed traceback:\n{detailed_error}")
    finally:
        # 8. Process and Save Results
        if game and game_history and agent_manager: # Ensure game object and history exist
            logger.info("Game loop finished or interrupted. Processing final results...")
            results_processor = GameResultsProcessor(config)
            results_processor.log_final_results(game)
            if config.log_to_file:
                results_processor.save_game_state(game, game_history) 
                if agent_manager.agents: # Check if agents were initialized
                    results_processor.save_agent_manifestos(agent_manager.agents)
                else:
                    logger.warning("AgentManager agents not initialized, skipping manifesto saving.")
            logger.info(f"Results processing complete. Total game time: {time.time() - start_time:.2f} seconds.")
            logger.info(f"Output files are located in: {config.game_id_specific_log_dir}")
        else:
            logger.error("Game object, game history, or agent manager was not initialized. No results to save.")

        logger.info("Diplomacy game simulation ended.")

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except Exception as e:
        # Catch-all for top-level errors not caught in main's try-except
        # This is important if main() itself raises an error before its own try-block
        # or if asyncio.run() fails for some reason.
        initial_logger = logging.getLogger(__name__) # Use a basic logger if setup_logging failed
        initial_logger.error(f"Critical error in asyncio.run(main()): {e}", exc_info=True)
        detailed_traceback = traceback.format_exc()
        initial_logger.error(f"Detailed traceback:\n{detailed_traceback}")
        sys.exit(1) # Exit with an error code



================================================
File: test_lm_game.py
================================================
#!/usr/bin/env python3
"""
Modified version of lm_game.py for testing first round functionality.
This allows stepping through just the first round to validate API calls.
"""

import asyncio
import logging
import os
import time
from typing import List, Optional # Added Any for mock args
from unittest.mock import patch
import pytest # Added pytest
import json # Added json import

import dotenv
from diplomacy import Game

# New refactored components
from ai_diplomacy.game_config import GameConfig
from ai_diplomacy.logging_setup import setup_logging
from ai_diplomacy.agent_manager import AgentManager
from ai_diplomacy.game_history import GameHistory
from ai_diplomacy.utils import get_valid_orders, gather_possible_orders, LLMInvalidOutputError


# Suppress warnings
os.environ["GRPC_PYTHON_LOG_LEVEL"] = "40"
os.environ["GRPC_VERBOSITY"] = "ERROR"
os.environ["ABSL_MIN_LOG_LEVEL"] = "2"
os.environ["GRPC_POLL_STRATEGY"] = "poll"

dotenv.load_dotenv()

logger = logging.getLogger(__name__)

# Default model for live tests - MAKE SURE THIS IS A VALID, ACCESSIBLE MODEL
LIVE_MODEL_ID = "gemma3:latest" # Or use an environment variable

class MockArgs:
    """Helper class to simulate argparse.Namespace for GameConfig."""
    def __init__(self, **kwargs):
        self.__dict__.update(kwargs)

def _prepare_config_for_test(execution_mode: str, test_powers_str: str, model_ids_str: Optional[str] = None, 
                             num_players: int = 1, num_sequential: int = 2, max_concurrent: int = 2) -> GameConfig:
    """Helper function to create GameConfig for tests."""
    
    use_mocks = (execution_mode == "mock")
    
    # Determine model_ids based on execution_mode
    if use_mocks:
        # For mock mode, use placeholder model IDs that match what mock logic might expect
        # or simply ensure they are syntactically valid if not used by mock logic.
        actual_model_ids_str = model_ids_str if model_ids_str else "mock_model_1,mock_model_2"
    else:
        # For live mode, use the actual live model ID.
        # If specific models were passed for live, use them, otherwise default to LIVE_MODEL_ID.
        actual_model_ids_str = model_ids_str if model_ids_str else LIVE_MODEL_ID
        # Ensure all powers in live mode use a valid live model if multiple powers are tested
        num_test_powers = len(test_powers_str.split(','))
        if ',' not in actual_model_ids_str and num_test_powers > 1 : # Only one model ID provided for multiple powers
             actual_model_ids_str = ','.join([LIVE_MODEL_ID] * num_test_powers)


    parsed_model_ids = [m.strip() for m in actual_model_ids_str.split(',')]
    
    test_powers_list = [p.strip().upper() for p in test_powers_str.split(',')]
    num_models = len(parsed_model_ids)
    fixed_models = [parsed_model_ids[i % num_models] for i in range(len(test_powers_list))]

    args = MockArgs(
        use_mocks=use_mocks,
        dev_mode=True, # Default from original script
        game_id_prefix=f"pytest_{execution_mode}",
        log_level="INFO",
        log_to_file=True,
        log_dir="./pytest_logs",
        test_powers=test_powers_str,
        model_ids=parsed_model_ids, # Store as list
        fixed_models=fixed_models,
        num_players=num_players,
        num_sequential=num_sequential,
        max_concurrent=max_concurrent,
        # Defaults for other GameConfig fields not covered by original args
        power_name=None, # From original args, default None
        game_id=f"pytest_{execution_mode}_{int(time.time())}",
        exclude_powers=None,
        max_years=None,
        perform_planning_phase=False,
        num_negotiation_rounds=0,
        negotiation_style="simultaneous",
        randomize_fixed_models=False
    )
    
    config = GameConfig(args) # type: ignore
    setup_logging(config) # Setup logging for each test run based on its config
    return config

class GameTester:
    """Test class for stepping through game functionality."""
    
    def __init__(self, config: GameConfig):
        self.config = config
        self.game = None
        self.game_history = None
        self.agent_manager = None
        
    async def setup_game(self):
        """Set up the game environment."""
        logger.info(f"Setting up test game environment (Mocks: {self.config.args.use_mocks})...")
        self.game = Game()
        self.game_history = GameHistory()
        self.agent_manager = AgentManager(self.config)
        logger.info(f"Game setup complete. Current phase: {self.game.current_short_phase}")
        
    async def test_single_round(self, test_powers: List[str]):
        """Test a single round of order generation."""
        logger.info(f"Testing single round for powers: {test_powers} (Mocks: {self.config.args.use_mocks})")
        powers_and_models = {}
        fixed_models_list = self.config.args.fixed_models
        for i, power in enumerate(test_powers):
            powers_and_models[power] = fixed_models_list[i]
        self.agent_manager.initialize_agents(powers_and_models)
        if not self.agent_manager.agents:
            logger.error("❌ No agents were initialized")
            return False
        logger.info(f"✅ Initialized {len(self.agent_manager.agents)} agents")
        success_count = 0
        total_count = len(test_powers)
        for power_name in test_powers:
            if power_name not in self.agent_manager.agents:
                logger.error(f"❌ Agent for {power_name} not found")
                continue
            logger.info(f"\n--- Testing {power_name} ---")
            try:
                success = await self.test_power_order_generation(power_name)
                if success:
                    success_count += 1
                    logger.info(f"✅ {power_name}: Order generation successful")
                else:
                    logger.error(f"❌ {power_name}: Order generation failed")
            except Exception as e:
                logger.error(f"❌ {power_name}: Exception during test - {e}", exc_info=True)
        success_rate = (success_count / total_count) * 100 if total_count > 0 else 0
        logger.info(f"\n📊 Single round test results: {success_count}/{total_count} successful ({success_rate:.1f}%)")
        return success_count == total_count
    
    async def _get_mocked_llm_call_internal(self, power_name_for_mock: str, *args, **kwargs):
        """Helper to provide a mocked llm_call_internal behavior for a specific power."""
        mock_orders_db = {
            "FRANCE": ["A PAR H", "A MAR H", "F BRE H"],
            "GERMANY": ["A BER H", "A MUN H", "F KIE H"],
            "ENGLAND": ["F LON H", "F EDI H", "A LVP H"],
        }
        selected_orders = mock_orders_db.get(power_name_for_mock, [f"A {power_name_for_mock[:3].upper()} H"])
        # Create a dictionary, then dump it to a JSON string.
        mock_orders_dict = {"orders": selected_orders}
        mock_response_json_string = json.dumps(mock_orders_dict)

        mock_full_response = f"Reasoning:\n- Mock reasoning for {power_name_for_mock}\n- These are test orders for validation\n\nPARSABLE OUTPUT:\n{mock_response_json_string}"
        return mock_full_response

    async def test_power_order_generation(self, power_name: str) -> bool:
        """Tests order generation for a single power."""
        agent = self.agent_manager.get_agent(power_name)
        if not agent:
            logger.error(f"Agent for {power_name} not found during order generation test.")
            return False
        current_phase = self.game.current_short_phase
        logger.info(f"[{power_name}] Current phase for order generation: {current_phase}")
        board_state = self.game.get_state()
        possible_orders = gather_possible_orders(self.game, power_name)
        logger.info(f"Possible orders for {power_name}: {list(possible_orders.keys())}")
        log_file_path = os.path.join(self.config.game_id_specific_log_dir, "test_orders.csv")
        os.makedirs(os.path.dirname(log_file_path), exist_ok=True)
        logger.info(f"🚀 Generating orders for {power_name}...")
        start_time = time.time()
        
        # Ensure agent.model_id, agent.system_prompt etc are correctly populated by AgentManager
        orders_callable = get_valid_orders(
            game=self.game, model_id=agent.model_id, agent_system_prompt=agent.system_prompt,
            board_state=board_state, power_name=power_name, possible_orders=possible_orders,
            game_history=self.game_history, game_id=self.config.game_id,
            config=self.config, agent_goals=agent.goals, agent_relationships=agent.relationships,
            agent_private_diary_str=agent.format_private_diary_for_prompt(),
            log_file_path=log_file_path, phase=current_phase
        )
        orders = None
        try:
            if self.config.args.use_mocks:
                async def mock_side_effect(*args_inner, **kwargs_inner): # Renamed args to avoid clash
                    return await self._get_mocked_llm_call_internal(power_name, *args_inner, **kwargs_inner)
                with patch('ai_diplomacy.services.llm_coordinator.llm_call_internal', side_effect=mock_side_effect):
                    orders = await orders_callable
            else:
                orders = await orders_callable
        except LLMInvalidOutputError as e:
            logger.error(f"DEV_MODE: LLMInvalidOutputError for {power_name} ({agent.model_id}): {e}")
            # Log details as before
            return False
            
        end_time = time.time()
        duration = end_time - start_time
        logger.info(f"⏱️  Order generation for {power_name} took {duration:.2f} seconds")
        logger.info(f"📋 Generated orders for {power_name}: {orders}")

        if self.config.args.use_mocks and orders is not None:
            mock_db = { "FRANCE": ["A PAR H", "A MAR H", "F BRE H"], "GERMANY": ["A BER H", "A MUN H", "F KIE H"], "ENGLAND": ["F LON H", "F EDI H", "A LVP H"]}
            expected_mocked_orders = mock_db.get(power_name, [f"A {power_name[:3].upper()} H"])
            assert sorted(orders) == sorted(expected_mocked_orders), f"Mock orders mismatch for {power_name}: expected {expected_mocked_orders}, got {orders}"
            
        if orders and len(orders) > 0:
            logger.info(f"✅ Successfully generated {len(orders)} orders for {power_name}")
            return True
        else:
            logger.warning(f"⚠️  No orders generated for {power_name}")
            return False

    async def test_sequential_calls(self, power_name: str, num_calls: int):
        """Test multiple sequential API calls."""
        logger.info(f"Testing {num_calls} sequential calls for {power_name} (Mocks: {self.config.args.use_mocks})")
        
        # Agent initialization is now expected to be handled by the calling test function's setup
        # or test_single_round if that's what sets up agents.
        # For this specific test, we ensure the agent for power_name is initialized.
        if power_name not in self.agent_manager.agents:
             # Attempt to initialize just this one agent if not already present
            logger.info(f"Agent for {power_name} not found, attempting initialization for sequential test.")
            try:
                original_test_powers = [p.strip().upper() for p in self.config.args.test_powers.split(',')]
                power_index = original_test_powers.index(power_name)
                model_to_use_for_power = self.config.args.fixed_models[power_index]
                self.agent_manager.initialize_agents({power_name: model_to_use_for_power})
            except (ValueError, IndexError) as e:
                 logger.error(f"❌ Failed to determine model for {power_name} for sequential test: {e}")
                 return False

        if power_name not in self.agent_manager.agents:
            logger.error(f"❌ Failed to initialize agent for {power_name} for sequential test.")
            return False
        
        success_count = 0
        for i in range(num_calls):
            logger.info(f"\n--- Sequential Call {i+1}/{num_calls} ---")
            success = await self.test_power_order_generation(power_name)
            if success:
                success_count += 1
                logger.info(f"✅ Call {i+1} for {power_name} succeeded")
            else:
                logger.warning(f"⚠️  Call {i+1} for {power_name} failed")
            if i < num_calls - 1:
                await asyncio.sleep(0.1 if self.config.args.use_mocks else 1)
        success_rate = (success_count / num_calls) * 100
        logger.info(f"\n📊 Sequential test results for {power_name}: {success_count}/{num_calls} successful ({success_rate:.1f}%)")
        return success_count == num_calls

    async def test_concurrent_calls(self, test_powers: List[str], max_concurrent: int):
        """Test concurrent API calls."""
        # Agent initialization is expected to be handled by the calling test function's setup.
        # This method will operate on the agents already initialized in self.agent_manager.
        
        concurrent_powers_to_test = [p for p in test_powers if p in self.agent_manager.agents][:max_concurrent]
        if not concurrent_powers_to_test:
            logger.error("❌ No agents available or initialized for concurrent test.")
            return False

        logger.info(f"Testing concurrent calls for powers: {concurrent_powers_to_test} (Mocks: {self.config.args.use_mocks})")
        
        tasks = [asyncio.create_task(self.test_power_order_generation(p_name)) for p_name in concurrent_powers_to_test]
        logger.info(f"🚀 Starting {len(tasks)} concurrent API calls...")
        start_time = time.time()
        results_list = await asyncio.gather(*tasks)
        end_time = time.time()
        duration = end_time - start_time
        results_map = {power_name: result for power_name, result in zip(concurrent_powers_to_test, results_list)}
        for p_name, res in results_map.items():
            logger.info(f"🏁 Concurrent result for {p_name}: {'Success' if res else 'Failed'}")
        success_count = sum(1 for res in results_list if res)
        total_count = len(concurrent_powers_to_test)
        logger.info(f"\n📊 Concurrent test results: {success_count}/{total_count} successful in {duration:.2f}s")
        return success_count == total_count

# --- Pytest Test Functions ---

@pytest.mark.asyncio
@pytest.mark.parametrize("execution_mode", ["mock"]) # Temporarily disable live for focus
async def test_single_round_scenario(execution_mode, request: pytest.FixtureRequest):
    # if execution_mode == "live": # Integration marker removed as live mode is disabled here
    #     request.applymarker(pytest.mark.integration)

    test_powers_str = "FRANCE,GERMANY"
    # For live mode, ensure enough models are specified or use the default LIVE_MODEL_ID for all
    model_ids_str = "mock_fr,mock_ge" if execution_mode == "mock" else f"{LIVE_MODEL_ID},{LIVE_MODEL_ID}"
    
    config = _prepare_config_for_test(execution_mode, test_powers_str, model_ids_str)
    
    tester = GameTester(config)
    await tester.setup_game() # Sets up agents based on config.args.test_powers and config.args.fixed_models
    
    # test_single_round expects a list of powers that are defined in config.args.test_powers
    # and have corresponding models in config.args.fixed_models
    # The setup_game initializes AgentManager, but test_single_round re-initializes agents
    # based on the test_powers list passed to it and models from config.args.fixed_models.
    # Ensure test_powers_list matches what fixed_models were set up for.
    test_powers_list = [p.strip().upper() for p in config.args.test_powers.split(',')]
    
    success = await tester.test_single_round(test_powers_list)
    assert success, f"Single round scenario failed in {execution_mode} mode."

@pytest.mark.asyncio
@pytest.mark.parametrize("execution_mode", ["mock"]) # Temporarily disable live for focus
async def test_order_generation_scenario(execution_mode, request: pytest.FixtureRequest):
    # if execution_mode == "live": # Integration marker removed as live mode is disabled here
    #     request.applymarker(pytest.mark.integration)

    power_to_test = "FRANCE"
    model_id_str = "mock_fr_single" if execution_mode == "mock" else LIVE_MODEL_ID
    config = _prepare_config_for_test(execution_mode, power_to_test, model_id_str)

    tester = GameTester(config)
    await tester.setup_game()
    
    # Initialize agent for the single power to test
    # test_power_order_generation expects agent to be in agent_manager
    assert isinstance(tester.agent_manager, AgentManager) # Ensure agent_manager is AgentManager
    tester.agent_manager.initialize_agents({power_to_test: config.args.fixed_models[0]})
    
    success = await tester.test_power_order_generation(power_to_test)
    assert success, f"Order generation scenario for {power_to_test} failed in {execution_mode} mode."

@pytest.mark.asyncio
@pytest.mark.parametrize("execution_mode", ["mock"]) # Temporarily disable live for focus
async def test_sequential_calls_scenario(execution_mode, request: pytest.FixtureRequest):
    # if execution_mode == "live": # Integration marker removed as live mode is disabled here
    #     request.applymarker(pytest.mark.integration)

    power_to_test = "FRANCE"
    num_sequential_calls = 2 # Reduced for test speed
    model_id_str = "mock_fr_seq" if execution_mode == "mock" else LIVE_MODEL_ID
    config = _prepare_config_for_test(execution_mode, power_to_test, model_id_str, num_sequential=num_sequential_calls)

    tester = GameTester(config)
    await tester.setup_game()

    # Agent for power_to_test needs to be initialized.
    # The test_sequential_calls method itself re-initializes the agent.
    # So, ensuring config.args.test_powers and config.args.fixed_models are correctly set up by _prepare_config_for_test is key.
    
    success = await tester.test_sequential_calls(power_to_test, num_sequential_calls)
    assert success, f"Sequential calls scenario for {power_to_test} failed in {execution_mode} mode."

@pytest.mark.asyncio
@pytest.mark.parametrize("execution_mode", ["mock"]) # Temporarily disable live for focus
async def test_concurrent_calls_scenario(execution_mode, request: pytest.FixtureRequest):
    # if execution_mode == "live": # Integration marker removed as live mode is disabled here
    #     request.applymarker(pytest.mark.integration)

    test_powers_str = "FRANCE,GERMANY"
    max_concurrent_calls = 2
     # For live mode, ensure enough models are specified or use the default LIVE_MODEL_ID for all
    model_ids_str = "mock_fr_con,mock_ge_con" if execution_mode == "mock" else f"{LIVE_MODEL_ID},{LIVE_MODEL_ID}"
    
    config = _prepare_config_for_test(execution_mode, test_powers_str, model_ids_str, max_concurrent=max_concurrent_calls)

    tester = GameTester(config)
    await tester.setup_game()

    # test_concurrent_calls expects agents to be initialized.
    # It will select from agents already in tester.agent_manager.agents.
    # The AgentManager is initialized in setup_game using config.args.test_powers and config.args.fixed_models.
    # We need to ensure the agents for test_powers_str are initialized.
    
    powers_to_test_list = [p.strip().upper() for p in test_powers_str.split(',')]
    
    # Initialize agents that will be used in the concurrent test
    powers_and_models_for_concurrent = {}
    for i, power_name in enumerate(powers_to_test_list):
        powers_and_models_for_concurrent[power_name] = config.args.fixed_models[i]
    assert isinstance(tester.agent_manager, AgentManager) # Ensure agent_manager is AgentManager
    tester.agent_manager.initialize_agents(powers_and_models_for_concurrent)

    success = await tester.test_concurrent_calls(powers_to_test_list, max_concurrent_calls)
    assert success, f"Concurrent calls scenario failed in {execution_mode} mode."


================================================
File: test_stage0.py
================================================
#!/usr/bin/env python3
"""
Test script for Stage 0 of the refactor.
Verifies that the new directory structure and basic components work.
"""

import logging
import pytest # Add pytest
from ai_diplomacy.core.state import PhaseState

from ai_diplomacy.agents.scripted_agent import ScriptedAgent
from ai_diplomacy.services.config import DiplomacyConfig, AgentConfig, GameConfig
from ai_diplomacy.services.llm_coordinator import LLMCoordinator

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def test_core_state():
    """Test PhaseState creation."""
    logger.info("Testing PhaseState creation...")
    
    # Create a minimal test phase state
    phase = PhaseState(
        phase_name="S1901M",
        year=1901,
        season="SPRING",
        phase_type="MOVEMENT",
        powers=frozenset(["FRANCE", "GERMANY"]),
        units={"FRANCE": ["A PAR", "F BRE"], "GERMANY": ["A BER", "F KIE"]},
        supply_centers={"FRANCE": ["PAR", "BRE", "MAR"], "GERMANY": ["BER", "KIE", "MUN"]}
    )
    
    assert phase.get_center_count("FRANCE") == 3
    assert phase.get_center_count("GERMANY") == 3
    assert not phase.is_power_eliminated("FRANCE")
    
    logger.info("✓ PhaseState working correctly")


@pytest.mark.asyncio
async def test_scripted_agent():
    """Test scripted agent functionality."""
    logger.info("Testing ScriptedAgent...")
    
    agent = ScriptedAgent("test-france", "FRANCE", "neutral")
    
    # Create test phase state
    phase = PhaseState(
        phase_name="S1901M",
        year=1901,
        season="SPRING",
        phase_type="MOVEMENT",
        powers=frozenset(["FRANCE", "GERMANY"]),
        units={"FRANCE": ["A PAR", "F BRE"], "GERMANY": ["A BER", "F KIE"]},
        supply_centers={"FRANCE": ["PAR", "BRE", "MAR"], "GERMANY": ["BER", "KIE", "MUN"]}
    )
    
    # Test order generation
    orders = await agent.decide_orders(phase)
    logger.info(f"Generated {len(orders)} orders: {[str(o) for o in orders]}")
    
    # Test message generation
    messages = await agent.negotiate(phase)
    logger.info(f"Generated {len(messages)} messages")
    
    # Test state update
    await agent.update_state(phase, [])
    
    logger.info("✓ ScriptedAgent working correctly")


def test_config():
    """Test configuration system."""
    logger.info("Testing configuration system...")
    
    # Test creating config from scratch
    game_config = GameConfig(token_budget=5000, use_mcp=False)
    agent_config = AgentConfig(country="FRANCE", type="llm", model_id="gpt-4o-mini")
    config = DiplomacyConfig(game=game_config, agents=[agent_config])
    
    assert config.game.token_budget == 5000
    agent_config = config.get_agent_config("FRANCE")
    assert agent_config is not None
    assert agent_config.model_id == "gpt-4o-mini"
    assert len(config.get_llm_agents()) == 1
    
    logger.info("✓ Configuration system working correctly")


def test_llm_coordinator():
    """Test LLM coordinator initialization."""
    logger.info("Testing LLM coordinator...")
    
    # Just test initialization for now
    coordinator = LLMCoordinator()
    assert coordinator is not None
    
    logger.info("✓ LLM coordinator initialized correctly")

# Removed main() function and if __name__ == "__main__": block
# Pytest will discover and run the test functions automatically.


================================================
File: test_stage1.py
================================================
#!/usr/bin/env python3
"""
Test script for Stage 1 of the refactor.
Verifies that the clean agent boundary works correctly.
"""

import logging
from ai_diplomacy.core.state import PhaseState
from ai_diplomacy.core.manager import GameEvent
from ai_diplomacy.agents.factory import AgentFactory
from ai_diplomacy.agents.llm_agent import LLMAgent
from ai_diplomacy.services.config import DiplomacyConfig, AgentConfig, GameConfig

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def test_agent_factory():
    """Test the agent factory creation."""
    logger.info("Testing AgentFactory...")
    
    factory = AgentFactory()
    
    # Test LLM agent creation
    llm_config = AgentConfig(country="FRANCE", type="llm", model_id="gpt-4o-mini")
    llm_agent = factory.create_agent("test-llm", "FRANCE", llm_config, "test-game")
    
    assert isinstance(llm_agent, LLMAgent)
    assert llm_agent.country == "FRANCE"
    assert llm_agent.config.model_id == "gpt-4o-mini"
    
    # Test scripted agent creation
    scripted_config = AgentConfig(country="GERMANY", type="scripted")
    scripted_agent = factory.create_agent("test-scripted", "GERMANY", scripted_config, "test-game")
    
    assert scripted_agent.country == "GERMANY"
    assert scripted_agent.get_agent_info()["type"] == "ScriptedAgent"
    
    logger.info("✓ AgentFactory working correctly")


def test_config_integration():
    """Test creating agents from full configuration."""
    logger.info("Testing configuration integration...")
    
    # Create a diplomacy configuration
    game_config = GameConfig(token_budget=5000, use_mcp=False)
    agents_config = [
        AgentConfig(country="FRANCE", type="llm", model_id="gpt-4o-mini"),
        AgentConfig(country="GERMANY", type="scripted"),
        AgentConfig(country="ENGLAND", type="llm", model_id="claude-3-haiku")
    ]
    config = DiplomacyConfig(game=game_config, agents=agents_config)
    
    # Create agents from config
    factory = AgentFactory()
    agents = factory.create_agents_from_config(config, "test-game")
    
    assert len(agents) == 3
    assert "FRANCE" in agents
    assert "GERMANY" in agents
    assert "ENGLAND" in agents
    
    # Verify agent types
    assert isinstance(agents["FRANCE"], LLMAgent)
    assert agents["GERMANY"].get_agent_info()["type"] == "ScriptedAgent"
    assert isinstance(agents["ENGLAND"], LLMAgent)
    
    logger.info("✓ Configuration integration working correctly")


def test_game_manager():
    """Test the core game manager."""
    logger.info("Testing GameManager...")
    
    # We need a mock diplomacy Game for testing
    # For now, let's test what we can without the actual game
    
    # Test GameEvent creation
    event = GameEvent(
        event_type="unit_lost",
        phase="S1901M",
        participants={"country": "FRANCE", "unit": "A PAR"},
        details={"unit_type": "A"}
    )
    
    assert event.event_type == "unit_lost"
    assert event.participants["country"] == "FRANCE"
    
    logger.info("✓ GameManager components working correctly")


def test_clean_boundaries():
    """Test that the clean boundaries are maintained."""
    logger.info("Testing clean boundaries...")
    
    # Test that PhaseState is immutable
    phase = PhaseState(
        phase_name="S1901M",
        year=1901,
        season="SPRING",
        phase_type="MOVEMENT",
        powers=frozenset(["FRANCE"]),
        units={"FRANCE": ["A PAR"]},
        supply_centers={"FRANCE": ["PAR"]}
    )
    
    # Try to modify it (should not work due to frozen=True)
    try:
        phase.year = 1902  # This should fail
        assert False, "PhaseState should be immutable"
    except Exception:
        pass  # Expected
    
    # Test that agents receive PhaseState, not Game objects
    config = AgentConfig(country="FRANCE", type="llm", model_id="gpt-4o-mini")
    agent = LLMAgent("test", "FRANCE", config)
    
    # Verify agent doesn't have direct game access
    assert not hasattr(agent, 'game')
    assert hasattr(agent, 'config')
    assert hasattr(agent, 'llm_coordinator')
    
    logger.info("✓ Clean boundaries maintained")

# Removed main() function and if __name__ == "__main__": block
# Pytest will discover and run the test functions automatically.


================================================
File: test_stage2.py
================================================
#!/usr/bin/env python3
"""
Test script for Stage 2 of the refactor.
Verifies that the pluggable context provider system works correctly.
"""

import logging
import pytest # Add pytest
from unittest.mock import patch
from ai_diplomacy.core.state import PhaseState
from ai_diplomacy.agents.factory import AgentFactory
from ai_diplomacy.agents.llm_agent import LLMAgent
from ai_diplomacy.agents.base import Order # Import Order
from ai_diplomacy.services.config import DiplomacyConfig, AgentConfig, GameConfig
from ai_diplomacy.services.context_provider import (
    ContextProviderFactory, 
    InlineContextProvider, 
    MCPContextProvider,
    ContextData
)

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def test_context_provider_factory():
    """Test the context provider factory."""
    logger.info("Testing ContextProviderFactory...")
    
    factory = ContextProviderFactory()
    
    # Test inline provider
    inline_provider = factory.get_provider("inline")
    assert isinstance(inline_provider, InlineContextProvider)
    assert inline_provider.is_available()
    
    # Test MCP provider (should fallback to inline since MCP client not configured)
    mcp_provider_fallback = factory.get_provider("mcp")
    assert isinstance(mcp_provider_fallback, InlineContextProvider)  # Should fallback
    
    # Test auto provider (should return inline since MCP not available)
    auto_provider = factory.get_provider("auto")
    assert isinstance(auto_provider, InlineContextProvider)
    
    # Test available providers
    available = factory.get_available_providers()
    assert "inline" in available
    
    logger.info("✓ ContextProviderFactory working correctly")


@pytest.mark.asyncio
async def test_inline_context_provider():
    """Test the inline context provider."""
    logger.info("Testing InlineContextProvider...")
    
    provider = InlineContextProvider()
    
    # Create test phase state
    phase_state = PhaseState(
        phase_name="S1901M",
        year=1901,
        season="SPRING",
        phase_type="MOVEMENT",
        powers=frozenset(["FRANCE", "GERMANY"]),
        units={"FRANCE": ["A PAR", "F BRE"], "GERMANY": ["A BER", "F KIE"]},
        supply_centers={"FRANCE": ["PAR", "BRE", "MAR"], "GERMANY": ["BER", "KIE", "MUN"]}
    )
    
    # Create test context data
    context_data = ContextData(
        phase_state=phase_state,
        possible_orders={"A PAR": ["A PAR H", "A PAR-BUR"], "F BRE": ["F BRE H", "F BRE-ENG"]},
        recent_messages="France to Germany: Hello!",
        strategic_analysis="Paris is well defended."
    )
    
    # Create test agent config
    agent_config = AgentConfig(country="FRANCE", type="llm", model_id="gpt-4o-mini")
    
    # Test context provision
    result = await provider.provide_context("test-agent", "FRANCE", context_data, agent_config)
    
    assert result["provider_type"] == "inline"
    assert "context_text" in result
    assert result["tools_available"] is False
    
    # Check that context contains expected information
    context_text = result["context_text"]
    assert "FRANCE" in context_text
    assert "S1901M" in context_text
    assert "A PAR" in context_text
    assert "F BRE" in context_text
    assert "Hello!" in context_text
    
    logger.info("✓ InlineContextProvider working correctly")


@pytest.mark.asyncio
async def test_mcp_context_provider():
    """Test the MCP context provider (should show tools are not available)."""
    logger.info("Testing MCPContextProvider...")
    
    provider = MCPContextProvider()
    
    # Create test data (same as inline test)
    phase_state = PhaseState(
        phase_name="S1901M",
        year=1901,
        season="SPRING",
        phase_type="MOVEMENT",
        powers=frozenset(["FRANCE", "GERMANY"]),
        units={"FRANCE": ["A PAR", "F BRE"], "GERMANY": ["A BER", "F KIE"]},
        supply_centers={"FRANCE": ["PAR", "BRE", "MAR"], "GERMANY": ["BER", "KIE", "MUN"]}
    )
    
    context_data = ContextData(
        phase_state=phase_state,
        possible_orders={"A PAR": ["A PAR H", "A PAR-BUR"]},
        recent_messages="Test message"
    )
    
    agent_config = AgentConfig(country="FRANCE", type="llm", model_id="gpt-4o-mini")
    
    # Test context provision (should show tools not available)
    result = await provider.provide_context("test-agent", "FRANCE", context_data, agent_config)
    
    assert result["provider_type"] == "mcp"
    assert result["tools_available"] is False
    assert "MCP tools not available" in result["context_text"]
    
    logger.info("✓ MCPContextProvider correctly shows tools not available")


def test_config_context_provider():
    """Test that agent configs specify context providers correctly."""
    logger.info("Testing context provider configuration...")
    
    # Test explicit inline config
    inline_config = AgentConfig(country="FRANCE", type="llm", model_id="gpt-4o-mini", context_provider="inline")
    assert inline_config.context_provider == "inline"
    
    # Test explicit MCP config
    mcp_config = AgentConfig(country="GERMANY", type="llm", model_id="claude-3-haiku", context_provider="mcp")
    assert mcp_config.context_provider == "mcp"
    
    # Test auto config (default)
    auto_config = AgentConfig(country="ENGLAND", type="llm", model_id="gpt-4o")
    assert auto_config.context_provider == "auto"
    
    # Test resolve_context_provider function
    from ai_diplomacy.services.config import resolve_context_provider
    
    # Tool-capable model should resolve to MCP
    tool_config = AgentConfig(country="RUSSIA", type="llm", model_id="gpt-4o", context_provider="auto")
    resolved = resolve_context_provider(tool_config)
    assert resolved == "mcp"
    
    # Non-tool model should resolve to inline
    simple_config = AgentConfig(country="ITALY", type="llm", model_id="ollama/llama3", context_provider="auto")
    resolved = resolve_context_provider(simple_config)
    assert resolved == "inline"
    
    logger.info("✓ Context provider configuration working correctly")


@pytest.mark.asyncio
async def test_agent_with_context_providers():
    """Test that agents work correctly with different context providers."""
    logger.info("Testing agents with context providers...")
    
    # Create agents with different context provider configs
    inline_config = AgentConfig(country="FRANCE", type="llm", model_id="gpt-4o-mini", context_provider="inline")
    mcp_config = AgentConfig(country="GERMANY", type="llm", model_id="gpt-4o", context_provider="mcp")
    auto_config = AgentConfig(country="ENGLAND", type="llm", model_id="claude-3-haiku", context_provider="auto")
    
    factory = AgentFactory()
    
    # Create agents
    inline_agent = factory.create_agent("inline-test", "FRANCE", inline_config, "test-game")
    mcp_agent = factory.create_agent("mcp-test", "GERMANY", mcp_config, "test-game")
    auto_agent = factory.create_agent("auto-test", "ENGLAND", auto_config, "test-game")
    
    # Check that agents have correct context providers
    assert isinstance(inline_agent, LLMAgent)
    assert inline_agent.resolved_context_provider_type == "inline"
    
    assert isinstance(mcp_agent, LLMAgent)
    assert mcp_agent.resolved_context_provider_type == "inline"  # Should fallback since MCP not available
    
    assert isinstance(auto_agent, LLMAgent)
    assert auto_agent.resolved_context_provider_type == "inline"  # Should fallback since MCP not available
    
    # Create test phase state
    phase_state = PhaseState(
        phase_name="S1901M",
        year=1901,
        season="SPRING",
        phase_type="MOVEMENT",
        powers=frozenset(["FRANCE", "GERMANY", "ENGLAND"]),
        units={"FRANCE": ["A PAR"], "GERMANY": ["A BER"], "ENGLAND": ["F LON"]},
        supply_centers={"FRANCE": ["PAR"], "GERMANY": ["BER"], "ENGLAND": ["LON"]}
    )
    
    # Test that agents can call decide_orders with context providers
    mock_llm_orders_string_output = '{"orders": ["A PAR H"]}'
    expected_agent_orders = [Order("A PAR H")]

    # Test inline_agent
    with patch('ai_diplomacy.services.llm_coordinator.llm_call_internal', return_value=mock_llm_orders_string_output) as mock_llm_call_inline:
        orders = await inline_agent.decide_orders(phase_state)
        assert orders == expected_agent_orders
        assert mock_llm_call_inline.call_count == 1
        logger.info(f"Inline agent generated {len(orders)} orders with mock")

    # Test mcp_agent
    with patch('ai_diplomacy.services.llm_coordinator.llm_call_internal', return_value=mock_llm_orders_string_output) as mock_llm_call_mcp:
        orders = await mcp_agent.decide_orders(phase_state)
        assert orders == expected_agent_orders
        assert mock_llm_call_mcp.call_count == 1 
        logger.info(f"MCP agent generated {len(orders)} orders with mock (expected fallback to inline)")

    # Test auto_agent
    with patch('ai_diplomacy.services.llm_coordinator.llm_call_internal', return_value=mock_llm_orders_string_output) as mock_llm_call_auto:
        orders = await auto_agent.decide_orders(phase_state)
        assert orders == expected_agent_orders
        assert mock_llm_call_auto.call_count == 1
        logger.info(f"Auto agent generated {len(orders)} orders with mock (expected fallback to inline)")
    
    logger.info("✓ Agents working correctly with mocked context providers and LLM calls")


def test_full_config_integration():
    """Test full configuration integration with context providers."""
    logger.info("Testing full configuration integration...")
    
    # Create a full diplomacy configuration with mixed context providers
    game_config = GameConfig(token_budget=5000, use_mcp=True)
    agents_config = [
        AgentConfig(country="FRANCE", type="llm", model_id="gpt-4o-mini", context_provider="inline"),
        AgentConfig(country="GERMANY", type="llm", model_id="gpt-4o", context_provider="mcp"),
        AgentConfig(country="ENGLAND", type="llm", model_id="claude-3-haiku", context_provider="auto"),
        AgentConfig(country="RUSSIA", type="scripted", context_provider="inline"),  # Scripted doesn't use context providers
    ]
    config = DiplomacyConfig(game=game_config, agents=agents_config)
    
    # Create agents from config
    factory = AgentFactory()
    agents = factory.create_agents_from_config(config, "test-game")
    
    assert len(agents) == 4
    assert "FRANCE" in agents
    assert "GERMANY" in agents
    assert "ENGLAND" in agents
    assert "RUSSIA" in agents
    
    # Check that LLM agents have correct context providers
    assert isinstance(agents["FRANCE"], LLMAgent)
    assert agents["FRANCE"].resolved_context_provider_type == "inline"
    
    assert isinstance(agents["GERMANY"], LLMAgent)  
    assert agents["GERMANY"].resolved_context_provider_type == "inline"  # Fallback since MCP not available
    
    assert isinstance(agents["ENGLAND"], LLMAgent)
    assert agents["ENGLAND"].resolved_context_provider_type == "inline"  # Should fallback since MCP not available
    
    # Scripted agent doesn't have context providers
    assert agents["RUSSIA"].get_agent_info()["type"] == "ScriptedAgent"
    
    logger.info("✓ Full configuration integration working correctly")

# Removed main() function and if __name__ == "__main__": block
# Pytest will discover and run the test functions automatically.

