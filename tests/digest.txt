Directory structure:
└── tests/
    ├── __init__.py
    ├── _shared_fixtures.py
    ├── conftest.py
    ├── test_agent_manager.py
    ├── test_game_history.py
    ├── test_game_results.py
    ├── test_llm_agent.py
    ├── test_lm_game.py
    ├── test_mocked_llm_calls.py
    ├── test_model_integration.py
    ├── test_stage0.py
    ├── test_stage1.py
    ├── test_stage2.py
    └── unit/
        ├── __init__.py
        ├── test_phase_parsing.py
        ├── __pycache__/
        └── orchestrators/
            ├── __init__.py
            ├── test_build_strategy.py
            ├── test_movement_strategy.py
            ├── test_retreat_strategy.py
            └── __pycache__/

================================================
File: __init__.py
================================================



================================================
File: _shared_fixtures.py
================================================
import argparse
from typing import Optional, List, Dict, Any
from datetime import datetime
import os

# Assuming GameConfig is imported from the correct path
# Adjust the import path if GameConfig is located elsewhere.
from ai_diplomacy.game_config import GameConfig, DEFAULT_LOG_LEVEL, DEFAULT_GAME_ID_PREFIX, DEFAULT_NUM_PLAYERS, DEFAULT_NUM_NEGOTIATION_ROUNDS, DEFAULT_NEGOTIATION_STYLE

# Default values for args that GameConfig expects
DEFAULT_ARGS_VALUES = {
    "power_name": None,
    "model_id": None,
    "num_players": DEFAULT_NUM_PLAYERS,
    "game_id_prefix": DEFAULT_GAME_ID_PREFIX,
    "log_level": DEFAULT_LOG_LEVEL,
    "perform_planning_phase": False,
    "num_negotiation_rounds": DEFAULT_NUM_NEGOTIATION_ROUNDS,
    "negotiation_style": DEFAULT_NEGOTIATION_STYLE,
    "fixed_models": None,
    "randomize_fixed_models": False,
    "exclude_powers": None,
    "max_years": None,
    "log_to_file": False,  # Changed to False for tests to avoid creating log files by default
    "dev_mode": False,
    "verbose_llm_debug": False,
    "max_diary_tokens": 6500,
    "models_config_file": "models.toml", # Default, can be overridden
    "game_id": None, # Will be auto-generated if None
    "log_dir": None, # GameConfig handles default log dir creation
    # The following are args used in some test setups, but not directly by GameConfig init
    # They are included here to allow tests to pass them through kwargs if needed elsewhere.
    "use_mocks": True, # Common in tests
    "test_powers": "FRANCE", # Example, specific tests might override
}

def create_game_config(**kwargs: Any) -> GameConfig:
    """
    Factory function to create a GameConfig instance with sensible defaults,
    allowing overrides via kwargs.
    """
    args_dict = DEFAULT_ARGS_VALUES.copy()
    args_dict.update(kwargs)

    # Ensure game_id is unique if not provided, to prevent clashes during parallel tests
    if args_dict.get("game_id") is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
        prefix = args_dict.get("game_id_prefix", DEFAULT_GAME_ID_PREFIX)
        args_dict["game_id"] = f"{prefix}_test_{timestamp}"
    
    # For tests, if log_to_file is True but log_dir is not specified, 
    # we should provide a default test log_dir to avoid cluttering the root directory.
    if args_dict.get("log_to_file") and args_dict.get("log_dir") is None:
        test_log_dir_base = os.path.join(os.getcwd(), "test_logs")
        # Further isolate by game_id to prevent concurrent write issues if tests run in parallel
        # GameConfig itself will append the game_id to base_log_dir, so we just provide the base.
        args_dict["log_dir"] = test_log_dir_base


    args = argparse.Namespace(**args_dict)
    
    # Before creating GameConfig, ensure the models.toml exists if GameConfig tries to load it,
    # or mock its loading path if it's not essential for the test.
    # For simplicity, we'll assume tests can manage this (e.g., by providing a dummy file or overriding models_config_path to None).
    # If models_config_file is None, GameConfig should handle it gracefully.
    if "models_config_file" in args_dict and args_dict["models_config_file"] is not None:
        if not os.path.exists(args.models_config_file) and args.models_config_file == "models.toml":
            # If the default "models.toml" is specified and doesn't exist, set it to None
            # to prevent GameConfig from failing. Tests requiring it should create a dummy file.
            # Or, better, they can override args.models_config_file to a specific test file.
            print(f"Warning: Default models_config_file '{args.models_config_file}' not found. Setting to None for GameConfig creation.")
            args.models_config_file = None


    return GameConfig(args)

# Example of a fixture using the factory:
# import pytest
# @pytest.fixture
# def game_config_fixture(tmp_path) -> GameConfig:
#     # Example: create a temporary log directory for this specific test
#     test_specific_log_dir = tmp_path / "test_logs"
#     test_specific_log_dir.mkdir()
#     return create_game_config(log_to_file=True, log_dir=str(test_specific_log_dir)) 


================================================
File: conftest.py
================================================
import pytest
from unittest.mock import Mock, AsyncMock
from ai_diplomacy.services.config import AgentConfig
from ai_diplomacy.services.llm_coordinator import LLMCoordinator
from ai_diplomacy.services.context_provider import (
    ContextProviderFactory,
    ContextProvider,
)
from ai_diplomacy.core.state import PhaseState
from datetime import datetime
from typing import List, Any, Optional, Dict
import logging
import os

# Import the shared factory
from ._shared_fixtures import create_game_config
from ai_diplomacy.game_config import GameConfig # Keep for type hinting if needed


@pytest.fixture
def agent_config():
    return AgentConfig(
        country="ENGLAND",
        type="llm",
        model_id="test_model",
        context_provider="inline",
    )


@pytest.fixture
def mock_llm_coordinator():
    return AsyncMock(spec=LLMCoordinator)


@pytest.fixture
def mock_context_provider():
    provider = AsyncMock(spec=ContextProvider)
    provider.provide_context = AsyncMock(
        return_value={
            "context_text": "Mocked context",
            "tools_available": False,
            "tools": [],
            "provider_type": "mock_inline",
        }
    )
    return provider


@pytest.fixture
def mock_context_provider_factory(mock_context_provider):
    factory = Mock(spec=ContextProviderFactory)
    factory.get_provider.return_value = mock_context_provider
    return factory


@pytest.fixture
def mock_phase_state():
    phase_state = AsyncMock(spec=PhaseState)
    phase_state.phase_name = "S1901M"
    phase_state.get_power_units = Mock(return_value=["A LON", "F EDI"])
    phase_state.get_power_centers = Mock(return_value=["LON", "EDI"])
    phase_state.is_game_over = False
    phase_state.powers = ["ENGLAND", "FRANCE", "GERMANY"]
    phase_state.is_power_eliminated = Mock(return_value=False)
    return phase_state


@pytest.fixture
def mock_load_prompt_file(mocker):
    # TODO(remove-when-fixed-upstream): If this mock is considered egregious because it patches a utility function,
    # consider refactoring LLMAgent to allow injecting the system prompt content directly for easier testing.
    return mocker.patch(
        "ai_diplomacy.llm_utils.load_prompt_file",
        return_value="Mocked system prompt",
        autospec=True,  # Adhering to custom instructions
    )


class MockDiplomacyAgent:
    def __init__(self, power_name, model_id="mock_model"):
        self.power_name = power_name
        self.model_id = model_id
        self.goals = [f"Take over the world ({power_name})", "Make friends"]
        self.relationships = {"OTHER_POWER": "Neutral"}
        self.private_journal = [
            f"Journal Entry 1 for {power_name}",
            f"Journal Entry 2 for {power_name}",
        ]
        self.private_diary = [f"[S1901M] Diary entry for {power_name}"]

    def get_agent_info(
        self,
    ):  # Added to match BaseAgent interface if needed by processor
        return {
            "agent_id": f"mock_agent_{self.power_name}",
            "country": self.power_name,
            "type": self.__class__.__name__,
            "model_id": self.model_id,
        }


class MockGameHistoryResults:
    def __init__(self):
        self.phases = [  # Simplified phase objects for testing to_dict fallback
            {"name": "SPRING 1901M", "orders_by_power": {"FRANCE": ["A PAR H"]}},
            {
                "name": "AUTUMN 1901M",
                "orders_by_power": {"FRANCE": ["A PAR - BUR"]},
            },
        ]

    def to_dict(self):  # Added to satisfy GameResultsProcessor's expectation
        return {"phases": self.phases}


class MockDiplomacyGame:  # Mock for diplomacy.Game
    def __init__(self):
        self.is_game_done = True  # Mark as done for saving state
        self._current_phase = "WINTER 1905"  # Example
        self._centers = {  # Example SC map
            "FRANCE": ["PAR", "MAR", "BRE", "SPA", "POR", "BEL", "HOL"],
            "ENGLAND": ["LON", "LVP", "EDI", "NWY", "SWE"],
            "GERMANY": ["BER", "MUN", "KIE", "DEN", "RUH", "WAR", "MOS"],
        }
        self._winners = ["GERMANY"]  # Example winner

    def get_current_phase(self):
        return self._current_phase

    def get_state(self):  # Corresponds to game.map.centers in some diplomacy versions
        return {
            "centers": self._centers
        }  # Or however the real Game object structures this

    def get_winners(self):
        return self._winners


@pytest.fixture
def mock_game_config_results(tmp_path) -> GameConfig:
    # Using tmp_path to ensure log files (if any created by GameConfig) go to a temp dir
    # The factory defaults log_to_file=False, but if a test overrides it, this is safer.
    return create_game_config(game_id="results_test_game", log_dir=str(tmp_path / "test_results_logs"))


@pytest.fixture
def mock_diplomacy_agent_france():
    return MockDiplomacyAgent("FRANCE")


@pytest.fixture
def mock_diplomacy_agent_germany():
    return MockDiplomacyAgent("GERMANY", model_id="gpt-4-mini")


@pytest.fixture
def mock_game_history_results():
    return MockGameHistoryResults()


@pytest.fixture
def mock_diplomacy_game():
    return MockDiplomacyGame()


# Mock classes from ai_diplomacy.phase_summary for testing purposes
# TODO: Consider moving these to a more specific fixture file e.g. tests/fixtures/phase_summary_fixtures.py


class MockLLMInterface_PhaseSummary:  # Renamed to avoid conflict if other MockLLMInterfaces exist
    def __init__(self, power_name="FRANCE"):
        self.power_name = power_name
        self.logger = logging.getLogger(f"MockLLMInterface_PhaseSummary.{power_name}")

    async def request(
        self,
        model_id,
        prompt_text,
        system_prompt_text,
        game_id,
        agent_name,
        phase_str,
        request_identifier,
    ):
        # This mock simulate the request method of LLMCoordinator,
        # as PhaseSummaryGenerator uses llm_coordinator.request
        self.logger.info(
            f"request called for {phase_str} by {agent_name} with prompt: {prompt_text[:50]}..."
        )
        return f"This is a generated summary for {agent_name} for phase {phase_str}. Events: ..."


class MockGame_PhaseSummary:  # Renamed to avoid conflict
    def __init__(self, current_phase_name="SPRING 1901M"):
        self.current_short_phase = current_phase_name
        self.powers = {"FRANCE": None, "GERMANY": None}  # Dummy powers

    def get_current_phase(self):  # Ensure this method exists
        return self.current_short_phase


class MockPhase_PhaseSummary:  # Renamed
    def __init__(self, name):
        self.name = name
        self.orders_by_power = {}
        self.messages = []
        self.phase_summaries = {}

    def add_phase_summary(self, power_name, summary):
        self.phase_summaries[power_name] = summary


class MockGameHistory_PhaseSummary:  # Renamed
    def __init__(self):
        self.phases_by_name: Dict[str, MockPhase_PhaseSummary] = {} # Use renamed
        self.current_phase_name: Optional[str] = None
        self.all_phases: List[MockPhase_PhaseSummary] = [] # Use renamed

    def add_phase(self, phase_name: str):
        if phase_name not in self.phases_by_name:
            phase = MockPhase_PhaseSummary(phase_name) # Use renamed
            self.phases_by_name[phase_name] = phase
            self.all_phases.append(phase)
            self.current_phase_name = phase_name

    def get_phase_by_name(
        self, name_to_find: str
    ) -> Optional[MockPhase_PhaseSummary]:  # Use renamed
        return self.phases_by_name.get(name_to_find)

    def get_current_phase(self) -> Optional[MockPhase_PhaseSummary]: # Use renamed
        if self.current_phase_name:
            return self.phases_by_name.get(self.current_phase_name)
        return None

    def get_messages_by_phase(self, phase_name: str) -> List[Any]:
        phase = self.get_phase_by_name(phase_name)
        return phase.messages if phase else []

    def add_phase_summary(self, phase_name: str, power_name: str, summary: str):
        phase = self.get_phase_by_name(phase_name)
        if phase:
            phase.add_phase_summary(power_name, summary)


@pytest.fixture
def mock_llm_interface_phase_summary():
    return MockLLMInterface_PhaseSummary()


@pytest.fixture
def mock_game_phase_summary():
    return MockGame_PhaseSummary()


@pytest.fixture
def mock_game_history_phase_summary():
    return MockGameHistory_PhaseSummary()


@pytest.fixture
def mock_game_config_phase_summary(tmp_path) -> GameConfig:
    # This fixture previously returned an instance of MockGameConfig_PhaseSummary.
    # Now it uses the factory, passing parameters that were specific to MockGameConfig_PhaseSummary.
    # The `power_name` and `model_id` were key customizations.
    # The llm_log_path was also overridden. GameConfig creates this path based on game_id and log_dir.
    # If a specific dummy path is needed, it might require post-modification or more complex factory options.
    # For now, let's rely on the factory's defaults and allow specific tests to override further if needed.
    # Note: The `agents` dict with `MockAgent_PhaseSummary` was part of the old mock config.
    # This is not something GameConfig itself holds directly in its constructor args.
    # If tests relied on config.agents, they will need to be adjusted or this mock needs to be more complex.
    # For now, creating a standard config. Tests can add mock agents to it if necessary.

    return create_game_config(
        game_id="test_phase_summary",
        power_name="FRANCE", # This was a param to MockGameConfig_PhaseSummary
        model_id="default_summary_model", # This was a param to MockGameConfig_PhaseSummary
        log_dir=str(tmp_path / "test_phase_summary_logs"),
        # llm_log_path was previously hardcoded to "dummy_llm_log.csv". 
        # The factory will generate one based on game_id and log_dir. If tests need a fixed one,
        # they might need to mock os.path.join or adjust assertions.
    )


# Mock classes from ai_diplomacy.logging_setup for testing purposes
# TODO: Consider moving these to a more specific fixture file e.g. tests/fixtures/logging_fixtures.py


class MockArgs_LoggingSetup:
    def __init__(
        self,
        log_level="DEBUG",
        game_id="test_log_game_conftest",
        log_to_file=True,
        log_dir=None,
    ):
        self.log_level = log_level
        self.game_id_prefix = "test_log_conftest"
        self.game_id = game_id
        # self.current_datetime_str = datetime.now().strftime("%Y%m%d_%H%M%S") # Not needed by MinimalGameConfig_LoggingSetup
        self.log_to_file = log_to_file
        self.log_dir = log_dir
        # Attributes GameConfig expects from args, that MinimalGameConfig_LoggingSetup might not use directly
        # but could be part of a fuller GameConfig mock if this were to replace it.
        self.power_name = None
        self.model_id = None
        self.num_players = 7
        self.perform_planning_phase = False
        self.num_negotiation_rounds = 3
        self.negotiation_style = "simultaneous"
        self.fixed_models = None
        self.randomize_fixed_models = False
        self.exclude_powers = None
        self.max_years = None
        self.dev_mode = False  # Added as GameConfig might expect it
        self.verbose_llm_debug = False  # Added as GameConfig might expect it
        self.max_diary_tokens = 6500  # Added
        self.models_config_file = "models.toml"  # Added


# This MinimalGameConfig_LoggingSetup is quite different from the main GameConfig
# as it only sets a few attributes. It's used by test_logging_setup.py.
# It does not call super().__init__(args) with an argparse.Namespace.
# For now, let this remain as is, as its usage is specific.
# If it were to be a full GameConfig, it would use the factory.
class MinimalGameConfig_LoggingSetup:
    """Minimal GameConfig for testing logging setup without full GameConfig overhead."""

    def __init__(
        self,
        log_level="DEBUG",
        game_id="test_log_game_conftest",
        log_to_file=True,
        log_dir=None,
        verbose_llm_debug=False, # Added verbose_llm_debug
    ):
        self.log_level = log_level.upper()
        self.game_id = game_id
        self.log_to_file = log_to_file
        self.current_datetime_str = datetime.now().strftime("%Y%m%d_%H%M%S")

        if log_dir is None:
            self.base_log_dir = os.path.join(os.getcwd(), "logs_conftest_minimal")
        else:
            self.base_log_dir = log_dir

        self.game_id_specific_log_dir = os.path.join(self.base_log_dir, self.game_id)
        self.general_log_path = os.path.join(
            self.game_id_specific_log_dir, f"{self.game_id}_general.log"
        )
        self.llm_log_path = os.path.join( # Added llm_log_path
            self.game_id_specific_log_dir, f"{self.game_id}_llm_interactions.csv"
        )
        self.verbose_llm_debug = verbose_llm_debug # Added verbose_llm_debug

        # Ensure log directory exists if logging to file
        if self.log_to_file:
            os.makedirs(self.game_id_specific_log_dir, exist_ok=True)


@pytest.fixture
def mock_args_logging_setup():
    return MockArgs_LoggingSetup()


@pytest.fixture
def minimal_game_config_logging_setup_debug_verbose_false(tmp_path):
    log_dir = tmp_path / "minimal_log_debug_vfalse"
    log_dir.mkdir()
    return MinimalGameConfig_LoggingSetup(log_level="DEBUG", log_to_file=True, log_dir=str(log_dir), verbose_llm_debug=False)


@pytest.fixture
def minimal_game_config_logging_setup_debug_verbose_true(tmp_path):
    log_dir = tmp_path / "minimal_log_debug_vtrue"
    log_dir.mkdir()
    return MinimalGameConfig_LoggingSetup(log_level="DEBUG", log_to_file=True, log_dir=str(log_dir), verbose_llm_debug=True)


@pytest.fixture
def minimal_game_config_logging_setup_info_verbose_false(tmp_path):
    log_dir = tmp_path / "minimal_log_info_vfalse"
    log_dir.mkdir()
    return MinimalGameConfig_LoggingSetup(log_level="INFO", log_to_file=True, log_dir=str(log_dir), verbose_llm_debug=False)



================================================
File: test_agent_manager.py
================================================
import logging
import pytest
from pathlib import Path
from ai_diplomacy.agent_manager import (
    AgentManager,
    DEFAULT_AGENT_MANAGER_FALLBACK_MODEL,
)
from ai_diplomacy.agents.llm_agent import LLMAgent
from ._shared_fixtures import create_game_config

ALL_POWERS_IN_GAME = [
    "AUSTRIA",
    "ENGLAND",
    "FRANCE",
    "GERMANY",
    "ITALY",
    "RUSSIA",
    "TURKEY",
]

logger = logging.getLogger(__name__)

TOML_CONTENT_TEST_8 = """
default_model = "toml_default_model"
[powers]
FRANCE = "toml_france_model"
GERMANY = "toml_germany_model"
"""

TOML_CONTENT_TEST_9 = """
[powers]
FRANCE = "toml_france_model_should_be_overridden"
"""

TOML_CONTENT_TEST_10 = """
default_model = "toml_default"
[powers]
AUSTRIA = "toml_austria"
ENGLAND = "toml_england"
FRANCE = "toml_france"
GERMANY = "toml_germany"
ITALY = "toml_italy"
RUSSIA = "toml_russia"
TURKEY = "toml_turkey"
"""

TOML_CONTENT_TEST_11 = """
default_model = "my_global_default_from_toml"
[powers]
"""

def _assert_test_1_basic_assignment(assigned, manager, config, all_powers):
    assert len(assigned) == 2
    assigned_model_values = list(assigned.values())
    assert "ollama/modelA" in assigned_model_values
    assert "ollama/modelB" in assigned_model_values

def _assert_test_2_primary_agent_specified(assigned, manager, config, all_powers):
    assert len(assigned) == 3
    assert assigned.get("FRANCE") == "gpt-4o"
    other_models_count = 0
    for power, model in assigned.items():
        if power != "FRANCE":
            other_models_count += 1
            assert (
                model == "ollama/modelC"
                or model == DEFAULT_AGENT_MANAGER_FALLBACK_MODEL
            )
    assert other_models_count == 2

def _assert_test_3_exclude_powers_randomize(assigned, manager, config, all_powers):
    assert len(assigned) == 2
    assert "ITALY" not in assigned
    assert "TURKEY" not in assigned
    for power_name in assigned.keys():
        if config.args.exclude_powers is not None:
            assert power_name not in config.args.exclude_powers

def _assert_test_4_not_enough_fixed_models(assigned, manager, config, all_powers):
    assert len(assigned) == 3
    models_assigned = list(assigned.values())
    assert models_assigned.count("only_one_model") == 3

def _assert_test_5_num_players_zero(assigned, manager, config, all_powers):
    assert len(assigned) == 0

def _assert_test_6_num_players_one_primary(assigned, manager, config, all_powers):
    assert len(assigned) == 1
    assert assigned.get("GERMANY") == "claude-3"
    manager.initialize_agents(assigned)
    assert manager.get_agent("FRANCE") is None

def _assert_test_7_primary_agent_excluded(assigned, manager, config, all_powers):
    assert "FRANCE" not in assigned
    assert len(assigned) == 1
    assigned_power = list(assigned.keys())[0]
    assert assigned_power != "FRANCE"
    assert assigned[assigned_power] == DEFAULT_AGENT_MANAGER_FALLBACK_MODEL
    manager.initialize_agents(assigned)
    assert manager.get_agent("FRANCE") is None

def _assert_test_8a_toml_respected_limited_players(assigned, manager, config, all_powers):
    assert config.default_model_from_config == "toml_default_model"
    assert config.power_model_assignments.get("FRANCE") == "toml_france_model"
    assert len(assigned) == 2
    chosen_powers = list(assigned.keys())
    if "FRANCE" in chosen_powers:
        assert assigned["FRANCE"] == "toml_france_model"
    if "GERMANY" in chosen_powers:
        assert assigned["GERMANY"] == "toml_germany_model"
    for power, model_id in assigned.items():
        if power not in ["FRANCE", "GERMANY"]:
            assert model_id == "toml_default_model"
        elif power == "FRANCE":
            assert model_id == "toml_france_model"
        elif power == "GERMANY":
            assert model_id == "toml_germany_model"

def _assert_test_8b_toml_respected_all_players(assigned, manager, config, all_powers):
    assert config.default_model_from_config == "toml_default_model"
    assert config.power_model_assignments.get("FRANCE") == "toml_france_model"
    assert assigned.get("FRANCE") == "toml_france_model"
    assert assigned.get("GERMANY") == "toml_germany_model"
    for power in all_powers:
        if power not in ["FRANCE", "GERMANY"]:
            assert assigned.get(power) == "toml_default_model"

def _assert_test_9_toml_cli_conflict(assigned, manager, config, all_powers):
    assert len(assigned) == 1
    assert assigned.get("FRANCE") == "cli_france_model_wins"

def _assert_test_10_num_players_limits_toml(assigned, manager, config, all_powers):
    assert len(assigned) == 3
    for power_name, model_id in assigned.items():
        assert model_id == f"toml_{power_name.lower()}"

def _assert_test_11_default_model_from_config(assigned, manager, config, all_powers):
    assert len(assigned) == 2
    for model_id in assigned.values():
        assert model_id == "my_global_default_from_toml"

def _assert_test_12_fallback_model_no_config_default(assigned, manager, config, all_powers):
    assert config.default_model_from_config is None
    assert len(assigned) == 1
    assert list(assigned.values())[0] == DEFAULT_AGENT_MANAGER_FALLBACK_MODEL

@pytest.fixture
def game_config_factory():
    return create_game_config

PARAMETRIZED_TEST_CASES = [
    ("test_1_basic_assignment", 2, None, None, ["ollama/modelA", "ollama/modelB"], None, False, None, _assert_test_1_basic_assignment),
    ("test_2_primary_agent_specified", 3, "FRANCE", "gpt-4o", ["ollama/modelC"], None, False, None, _assert_test_2_primary_agent_specified),
    ("test_3_exclude_powers_randomize", 2, None, None, ["modelX", "modelY", "modelZ"], ["ITALY", "TURKEY"], True, None, _assert_test_3_exclude_powers_randomize),
    ("test_4_not_enough_fixed_models", 3, None, None, ["only_one_model"], None, False, None, _assert_test_4_not_enough_fixed_models),
    ("test_5_num_players_zero", 0, None, None, None, None, False, None, _assert_test_5_num_players_zero),
    ("test_6_num_players_one_primary", 1, "GERMANY", "claude-3", None, None, False, None, _assert_test_6_num_players_one_primary),
    ("test_7_primary_agent_excluded", 1, "FRANCE", "gpt-4o", None, ["FRANCE"], False, None, _assert_test_7_primary_agent_excluded),
    ("test_8a_toml_limited_players", 2, None, None, None, None, False, TOML_CONTENT_TEST_8, _assert_test_8a_toml_respected_limited_players),
    ("test_8b_toml_all_players", 7, None, None, None, None, False, TOML_CONTENT_TEST_8, _assert_test_8b_toml_respected_all_players),
    ("test_9_toml_cli_conflict", 1, "FRANCE", "cli_france_model_wins", None, None, False, TOML_CONTENT_TEST_9, _assert_test_9_toml_cli_conflict),
    ("test_10_num_players_limits_toml", 3, None, None, None, None, False, TOML_CONTENT_TEST_10, _assert_test_10_num_players_limits_toml),
    ("test_11_default_model_from_config", 2, None, None, None, None, False, TOML_CONTENT_TEST_11, _assert_test_11_default_model_from_config),
    ("test_12_fallback_model_no_default", 1, None, None, None, None, False, None, _assert_test_12_fallback_model_no_config_default),
]

@pytest.mark.parametrize(
    "test_id, num_players, cli_power_name, cli_model_id, fixed_models, exclude_powers, randomize_fixed, toml_content, assertion_fn",
    PARAMETRIZED_TEST_CASES,
    ids=[case[0] for case in PARAMETRIZED_TEST_CASES]
)
def test_assign_models_parametrized(
    game_config_factory, tmp_path,
    test_id, num_players, cli_power_name, cli_model_id, fixed_models, exclude_powers, randomize_fixed, toml_content, assertion_fn
):
    logger.info(f"--- Running Parametrized Test Case: {test_id} ---")
    models_config_file_path = None
    if toml_content:
        models_config_file_path = tmp_path / f"{test_id}_models.toml"
        with open(models_config_file_path, "w") as f:
            f.write(toml_content)
    config = game_config_factory(
        num_players=num_players,
        power_name=cli_power_name,
        model_id=cli_model_id,
        fixed_models=fixed_models,
        exclude_powers=exclude_powers,
        randomize_fixed_models=randomize_fixed,
        models_config_file=str(models_config_file_path) if models_config_file_path else None,
        log_to_file=False
    )
    manager = AgentManager(config)
    assigned = manager.assign_models(ALL_POWERS_IN_GAME)
    logger.info(f"Test Case {test_id} Assigned: {assigned}")
    assertion_fn(assigned, manager, config, ALL_POWERS_IN_GAME)
    if assigned:
        manager.initialize_agents(assigned)
        assert len(manager.agents) == len(assigned)
        for power_name, model_id_assigned in assigned.items():
            assert power_name in manager.agents
            agent = manager.get_agent(power_name)
            assert agent is not None
            assert agent.country == power_name
            assert isinstance(agent, LLMAgent)
            assert agent.model_id == model_id_assigned
    else:
        manager.initialize_agents(assigned)
        assert len(manager.agents) == 0

logger.info("--- All AgentManager tests collected (parametrized) ---")



================================================
File: test_game_history.py
================================================
import json
from ai_diplomacy.game_history import GameHistory, Phase


def test_game_history_to_dict_serialization():
    game_history = GameHistory()
    game_history.add_phase("S1901M")
    phase1 = game_history.get_phase_by_name("S1901M")
    assert phase1 is not None
    phase1.add_plan("FRANCE", "Plan for France in S1901M")
    phase1.add_message("ENGLAND", "FRANCE", "Hello from England in S1901M!")
    phase1.orders_by_power["ITALY"].extend(["F ROM - NAP"])
    phase1.results_by_power["ITALY"].extend([["SUCCESSFUL"]])
    phase1.phase_summaries["GERMANY"] = "Germany did well in S1901M."
    phase1.experience_updates["RUSSIA"] = "Learned about betrayal in S1901M."
    game_history.add_phase("F1901M")
    phase2 = game_history.get_phase_by_name("F1901M")
    assert phase2 is not None
    phase2.add_plan("AUSTRIA", "Plan for Austria in F1901M")
    phase2.add_message("TURKEY", "GLOBAL", "Turkey's global message in F1901M.")
    phase2.orders_by_power["ENGLAND"].extend(["A LON H", "F EDI S A LON H"])
    phase2.results_by_power["ENGLAND"].extend([["SUCCESSFUL"], ["SUCCESSFUL"]])
    phase2.phase_summaries["FRANCE"] = "France struggled in F1901M."
    phase2.experience_updates["ITALY"] = "Italy gained valuable experience in F1901M."
    history_dict = game_history.to_dict()
    assert isinstance(history_dict, dict)
    assert "phases" in history_dict
    assert isinstance(history_dict["phases"], list)
    assert len(history_dict["phases"]) == 2
    phase_1_dict = history_dict["phases"][0]
    assert phase_1_dict["name"] == "S1901M"
    assert "plans" in phase_1_dict and isinstance(phase_1_dict["plans"], dict)
    assert phase_1_dict["plans"]["FRANCE"] == "Plan for France in S1901M"
    assert "messages" in phase_1_dict and isinstance(phase_1_dict["messages"], list)
    assert len(phase_1_dict["messages"]) == 1
    message_1_dict = phase_1_dict["messages"][0]
    assert message_1_dict["sender"] == "ENGLAND"
    assert message_1_dict["recipient"] == "FRANCE"
    assert message_1_dict["content"] == "Hello from England in S1901M!"
    assert "orders_by_power" in phase_1_dict and isinstance(
        phase_1_dict["orders_by_power"], dict
    )
    assert phase_1_dict["orders_by_power"]["ITALY"] == ["F ROM - NAP"]
    assert "results_by_power" in phase_1_dict and isinstance(
        phase_1_dict["results_by_power"], dict
    )
    assert phase_1_dict["results_by_power"]["ITALY"] == [["SUCCESSFUL"]]
    assert "phase_summaries" in phase_1_dict and isinstance(
        phase_1_dict["phase_summaries"], dict
    )
    assert phase_1_dict["phase_summaries"]["GERMANY"] == "Germany did well in S1901M."
    assert "experience_updates" in phase_1_dict and isinstance(
        phase_1_dict["experience_updates"], dict
    )
    assert (
        phase_1_dict["experience_updates"]["RUSSIA"]
        == "Learned about betrayal in S1901M."
    )
    phase_2_dict = history_dict["phases"][1]
    assert phase_2_dict["name"] == "F1901M"
    assert phase_2_dict["plans"]["AUSTRIA"] == "Plan for Austria in F1901M"
    assert phase_2_dict["messages"][0]["sender"] == "TURKEY"
    json_string = json.dumps(
        history_dict, indent=2
    )
    assert isinstance(json_string, str)
    reloaded_dict = json.loads(json_string)
    assert reloaded_dict == history_dict


def test_game_history_to_dict_empty():
    game_history = GameHistory()
    history_dict = game_history.to_dict()
    assert isinstance(history_dict, dict)
    assert "phases" in history_dict
    assert isinstance(history_dict["phases"], list)
    assert len(history_dict["phases"]) == 0

    # Test JSON serialization for empty history
    json_string = json.dumps(history_dict)
    assert isinstance(json_string, str)
    reloaded_dict = json.loads(json_string)
    assert reloaded_dict == history_dict


def test_phase_add_orders_internal_consistency():
    """
    This test is more about the Phase.add_orders method if it's used for populating,
    but to_dict relies on consistent internal structure.
    """
    phase = Phase(name="T1900M")
    # Phase.add_orders(power, orders, results)
    phase.add_orders("FRANCE", ["A PAR H"], [["SUCCESSFUL"]])
    phase.add_orders("FRANCE", ["F BRE - MAO"], [["BOUNCED", "F ENG - MAO"]])

    phase_dict = {
        "name": phase.name,
        "plans": dict(phase.plans),
        "messages": [
            vars(msg) for msg in phase.messages
        ],  # vars() works for simple dataclasses
        "orders_by_power": {p: list(o) for p, o in phase.orders_by_power.items()},
        "results_by_power": {p: list(r) for p, r in phase.results_by_power.items()},
        "phase_summaries": dict(phase.phase_summaries),
        "experience_updates": dict(phase.experience_updates),
    }

    assert phase_dict["orders_by_power"]["FRANCE"] == ["A PAR H", "F BRE - MAO"]
    assert phase_dict["results_by_power"]["FRANCE"] == [
        ["SUCCESSFUL"],
        ["BOUNCED", "F ENG - MAO"],
    ]

    # Test with GameHistory.to_dict()
    gh = GameHistory()
    gh.phases.append(phase)
    gh_dict = gh.to_dict()

    assert gh_dict["phases"][0]["orders_by_power"]["FRANCE"] == [
        "A PAR H",
        "F BRE - MAO",
    ]
    assert gh_dict["phases"][0]["results_by_power"]["FRANCE"] == [
        ["SUCCESSFUL"],
        ["BOUNCED", "F ENG - MAO"],
    ]


# Test with empty orders/results lists for a power
def test_game_history_to_dict_empty_orders_for_power():
    game_history = GameHistory()
    game_history.add_phase("S1901M")
    phase1 = game_history.get_phase_by_name("S1901M")
    assert phase1 is not None

    # Add a power but no orders for it
    phase1.orders_by_power["AUSTRIA"] = []  # Explicitly empty
    phase1.results_by_power["AUSTRIA"] = []

    history_dict = game_history.to_dict()
    phase_1_dict = history_dict["phases"][0]

    assert "AUSTRIA" in phase_1_dict["orders_by_power"]
    assert phase_1_dict["orders_by_power"]["AUSTRIA"] == []
    assert "AUSTRIA" in phase_1_dict["results_by_power"]
    assert phase_1_dict["results_by_power"]["AUSTRIA"] == []

    json_string = json.dumps(history_dict)
    reloaded_dict = json.loads(json_string)
    assert reloaded_dict == history_dict


# Test with a phase that has no messages, plans, etc.
def test_game_history_to_dict_empty_phase_fields():
    game_history = GameHistory()
    game_history.add_phase("S1901M")  # Phase with no sub-data

    history_dict = game_history.to_dict()
    phase_1_dict = history_dict["phases"][0]

    assert phase_1_dict["name"] == "S1901M"
    assert phase_1_dict["plans"] == {}
    assert phase_1_dict["messages"] == []
    assert phase_1_dict["orders_by_power"] == {}  # defaultdict(list) becomes {}
    assert phase_1_dict["results_by_power"] == {}  # defaultdict(list) becomes {}
    assert phase_1_dict["phase_summaries"] == {}
    assert phase_1_dict["experience_updates"] == {}

    json_string = json.dumps(history_dict)
    reloaded_dict = json.loads(json_string)
    assert reloaded_dict == history_dict



================================================
File: test_game_results.py
================================================
import json
import os
from unittest.mock import MagicMock, mock_open, patch

from ai_diplomacy.game_results import GameResultsProcessor
from ai_diplomacy.game_history import GameHistory  # For creating mock GameHistory
from ai_diplomacy.game_config import GameConfig  # For mock GameConfig

# from diplomacy import Game # For type hinting mock_game if needed, but MagicMock is often sufficient


def test_save_game_state_writes_history_json():
    """
    Test that save_game_state calls game_history.to_dict() and writes its output to a JSON file.
    """
    # 1. Create Mock Objects
    mock_game_history = GameHistory()
    mock_game_history.add_phase("S1901M")
    phase1 = mock_game_history.get_phase_by_name("S1901M")
    assert phase1 is not None
    phase1.add_plan("FRANCE", "Test Plan S1901M")
    phase1.add_message("ENGLAND", "FRANCE", "Test Message S1901M")
    phase1.orders_by_power["ITALY"] = [
        "F ROM - NAP"
    ]  # Directly populating for simplicity
    phase1.results_by_power["ITALY"] = [["SUCCESSFUL"]]

    # Mock GameConfig
    # We need a GameConfig that provides paths.
    # Let's mock the args that GameConfig constructor expects.
    mock_cli_args = MagicMock()
    mock_cli_args.log_to_file = True
    mock_cli_args.game_id = "test_game_123"
    mock_cli_args.game_id_prefix = (
        "test_prefix"  # GameConfig uses this if game_id is None
    )
    mock_cli_args.log_dir = "dummy_base_log_dir"  # GameConfig will create subdirs here
    # Add any other attributes GameConfig's __init__ might access from args
    mock_cli_args.power_name = None
    mock_cli_args.model_id = None
    mock_cli_args.num_players = 7
    mock_cli_args.log_level = "INFO"
    mock_cli_args.perform_planning_phase = False
    mock_cli_args.num_negotiation_rounds = 1
    mock_cli_args.negotiation_style = "simultaneous"
    mock_cli_args.fixed_models = None
    mock_cli_args.randomize_fixed_models = False
    mock_cli_args.exclude_powers = None
    mock_cli_args.max_years = None
    mock_cli_args.dev_mode = False
    mock_cli_args.verbose_llm_debug = False
    mock_cli_args.max_diary_tokens = 6500
    mock_cli_args.models_config_file = (
        "models.toml"  # So it doesn't try to load a real one and fail tests
    )

    # Patch os.path.exists for models.toml to avoid trying to load it
    with patch("os.path.exists", return_value=False):
        mock_game_config = GameConfig(mock_cli_args)

    # Ensure results_dir is set correctly for the test
    expected_results_dir = os.path.join(
        mock_game_config.game_id_specific_log_dir, "results"
    )
    mock_game_config.results_dir = (
        expected_results_dir  # Override if necessary, though GameConfig should set it
    )

    # Mock diplomacy.Game object
    mock_game_instance = MagicMock()
    # Set attributes on mock_game_instance that save_game_state might access
    # For saving game state JSON (to_saved_game_format part)
    mock_game_instance.is_game_done = True
    # Mock to_saved_game_format if it's complex or from external lib not easily mocked
    # For this test, we are focusing on the history part.
    # We can make to_saved_game_format return a simple string.

    # 2. Patch builtins.open and os.makedirs
    with (
        patch("builtins.open", new_callable=mock_open) as mock_file_open,
        patch("os.makedirs") as mock_makedirs,
    ):  # os.makedirs is called by GameConfig and GameResultsProcessor
        # Instantiate GameResultsProcessor
        results_processor = GameResultsProcessor(mock_game_config)

        # Mock to_saved_game_format specifically for the call within save_game_state
        with patch(
            "ai_diplomacy.game_results.to_saved_game_format",
            return_value='{"mock_game_state": "data"}',
        ):
            # 3. Call save_game_state
            results_processor.save_game_state(mock_game_instance, mock_game_history)

            # 4. Assert os.makedirs was called (by GameConfig and potentially by save_game_state)
            # GameConfig creates results_dir if log_to_file is True
            mock_makedirs.assert_any_call(expected_results_dir, exist_ok=True)

            # 5. Assert open was called for the history JSON file
            # Expected path: os.path.join(mock_game_config.results_dir, f"{mock_game_config.game_id}_game_history.json")
            expected_history_filepath = os.path.join(
                expected_results_dir, f"{mock_game_config.game_id}_game_history.json"
            )

            # Check if open was called for the history file
            # It's also called for the final_state.json, so we look for the specific call
            found_history_file_call = False

            for call_args in mock_file_open.call_args_list:
                if call_args[0][0] == expected_history_filepath:
                    assert call_args[0][1] == "w"  # Mode 'w'
                    assert call_args[1]["encoding"] == "utf-8"
                    found_history_file_call = True
                    break
            assert found_history_file_call, (
                f"History file {expected_history_filepath} was not opened."
            )

            # 6. Assert json.dump or write content for the history file
            # Find the write call associated with the history file
            # This assumes json.dump uses the write method of the file handle from open()

            # Get the file handle that was used for the history file
            for call in mock_file_open.call_args_list:
                if call[0][0] == expected_history_filepath:
                    # The mock_open().write calls are on the instance returned by mock_file_open()
                    # when it was called for the history file.
                    # We need to find which mock_file_open() instance corresponds to the history file.
                    # This is tricky if multiple files are opened.
                    # A simpler way is to check the content passed to json.dump if we patch json.dump
                    break  # Found the open call, now need its handle's write calls.

            # Instead of inspecting mock_file_open's write calls directly (which can be complex
            # if multiple files are opened), we can patch json.dump for more direct assertion.

            # Re-run with json.dump patched
            with patch("json.dump") as mock_json_dump:
                # Re-call the function under test now that json.dump is patched
                # Need to reset mock_file_open if it's stateful across calls
                mock_file_open.reset_mock()

                # We need a fresh GameResultsProcessor or ensure state is clean if it's stateful
                # results_processor = GameResultsProcessor(mock_game_config) # Re-instantiate if needed

                with patch(
                    "ai_diplomacy.game_results.to_saved_game_format",
                    return_value='{"mock_game_state": "data"}',
                ):
                    results_processor.save_game_state(
                        mock_game_instance, mock_game_history
                    )

                # Find the call to json.dump that wrote the history data
                history_dump_call_args = None
                for call in mock_json_dump.call_args_list:
                    # The first argument to json.dump is the data, the second is the file handle
                    # We expect the data to be the dictionary from game_history.to_dict()
                    dumped_data = call[0][0]
                    if (
                        "phases" in dumped_data
                        and dumped_data["phases"][0]["name"] == "S1901M"
                    ):
                        history_dump_call_args = call
                        break

                assert history_dump_call_args is not None, (
                    "json.dump was not called with history data."
                )

                written_data_dict = history_dump_call_args[0][0]

                # Assert content based on mock_game_history.to_dict()
                expected_history_dict = mock_game_history.to_dict()
                assert written_data_dict == expected_history_dict
                assert "phases" in written_data_dict
                assert len(written_data_dict["phases"]) == 1
                assert written_data_dict["phases"][0]["name"] == "S1901M"
                assert (
                    written_data_dict["phases"][0]["plans"]["FRANCE"]
                    == "Test Plan S1901M"
                )
                assert (
                    written_data_dict["phases"][0]["messages"][0]["content"]
                    == "Test Message S1901M"
                )
                assert written_data_dict["phases"][0]["orders_by_power"]["ITALY"] == [
                    "F ROM - NAP"
                ]

    # Test when log_to_file is False
    # @patch("os.makedirs") # No need to patch if it shouldn't be called
    @patch("builtins.open", new_callable=mock_open)
    def test_save_game_state_log_to_file_false(mock_file_open_disabled):
        mock_cli_args = MagicMock()
        mock_cli_args.log_to_file = False  # Key change for this test
        mock_cli_args.game_id = "test_game_no_log"
        # Fill other necessary args for GameConfig constructor
        mock_cli_args.game_id_prefix = "test_prefix"
        mock_cli_args.log_dir = "dummy_base_log_dir"
        mock_cli_args.power_name = None
        mock_cli_args.model_id = None
        mock_cli_args.num_players = 7
        mock_cli_args.log_level = "INFO"
        mock_cli_args.perform_planning_phase = False
        mock_cli_args.num_negotiation_rounds = 1
        mock_cli_args.negotiation_style = "simultaneous"
        mock_cli_args.fixed_models = None
        mock_cli_args.randomize_fixed_models = False
        mock_cli_args.exclude_powers = None
        mock_cli_args.max_years = None
        mock_cli_args.dev_mode = False
        mock_cli_args.verbose_llm_debug = False
        mock_cli_args.max_diary_tokens = 6500
        mock_cli_args.models_config_file = "models.toml"

        with patch("os.path.exists", return_value=False):  # For models.toml
            mock_game_config_no_log = GameConfig(mock_cli_args)

        mock_game_instance_no_log = MagicMock()
        mock_game_history_no_log = GameHistory()  # Empty history is fine

        results_processor_no_log = GameResultsProcessor(mock_game_config_no_log)
        results_processor_no_log.save_game_state(
            mock_game_instance_no_log, mock_game_history_no_log
        )

        mock_file_open_disabled.assert_not_called()  # open should not be called
        # os.makedirs might still be called by GameConfig if log_to_file=True,
        # but GameResultsProcessor.save_game_state should not proceed to file operations.
        # GameConfig's makedirs is conditional on self.log_to_file for game_id_specific_log_dir
        # so if log_to_file is false from the start, even GameConfig might not call it for that.
        # This specific test is for save_game_state's behavior.

    # Test for GameHistory having a to_dict method (positive check)
    def test_game_history_has_to_dict_method():
        gh = GameHistory()
        assert hasattr(gh, "to_dict")
        assert callable(gh.to_dict)

    # Test for GameHistory to_dict with multiple phases and complex data
    def test_game_history_to_dict_complex():
        gh = GameHistory()
        gh.add_phase("S1901M")
        p1 = gh.get_phase_by_name("S1901M")
        assert p1 is not None
        p1.add_plan("FRANCE", "Invade GER")
        p1.add_message("FRANCE", "GERMANY", "I'm friendly")
        p1.orders_by_power["FRANCE"] = ["A PAR - BUR"]
        p1.results_by_power["FRANCE"] = [["SUCCESSFUL"]]
        p1.phase_summaries["FRANCE"] = "French mobilization"
        p1.experience_updates["FRANCE"] = "Learned about BUR"

        gh.add_phase("F1901M")
        p2 = gh.get_phase_by_name("F1901M")
        assert p2 is not None
        p2.add_plan("GERMANY", "Defend RUH")
        p2.add_message("GERMANY", "FRANCE", "Oh really?")
        p2.orders_by_power["GERMANY"] = ["A MUN H"]
        p2.results_by_power["GERMANY"] = [["SUCCESSFUL"]]

        data = gh.to_dict()
        assert len(data["phases"]) == 2
        assert data["phases"][0]["name"] == "S1901M"
        assert data["phases"][0]["plans"]["FRANCE"] == "Invade GER"
        assert data["phases"][1]["name"] == "F1901M"
        assert data["phases"][1]["messages"][0]["content"] == "Oh really?"

        # Check JSON serializability
        json_data = json.dumps(data)
        reloaded_data = json.loads(json_data)
        assert data == reloaded_data



================================================
File: test_llm_agent.py
================================================
import pytest
from unittest.mock import AsyncMock, Mock

from ai_diplomacy.agents.llm_agent import LLMAgent
from ai_diplomacy.agents.base import (
    Message,
    Order,
)
# Fixtures are now in conftest.py and will be auto-discovered by pytest
# No need to import AgentConfig, ContextProviderFactory, LLMCoordinator, PhaseState directly for mocking


# Helper function to create an agent instance with mocks, using fixtures
@pytest.fixture
def llm_agent(
    agent_config,
    mock_llm_coordinator,
    mock_context_provider_factory,
    mock_context_provider,
    mock_load_prompt_file,
):
    agent = LLMAgent(
        agent_id="test_agent",
        country="ENGLAND",
        config=agent_config,
        game_id="test_game",
        llm_coordinator=mock_llm_coordinator,
        context_provider_factory=mock_context_provider_factory,
    )
    # Ensure the agent uses the mocked context provider instance from the fixture
    agent.context_provider = mock_context_provider
    # Ensure system_prompt is loaded using the mocked load_prompt_file
    agent.system_prompt = agent._load_system_prompt()
    return agent


@pytest.mark.asyncio
async def test_negotiate_returns_messages_and_updates_diary(
    llm_agent, mock_llm_coordinator, mock_phase_state
):
    mock_llm_coordinator.call_json = AsyncMock(
        return_value={
            "messages": [
                {
                    "recipient": "FRANCE",
                    "content": "Hello France",
                    "message_type": "private",
                }
            ]
        }
    )

    # Clear diary before test if checking for specific entries
    llm_agent.private_diary = []

    messages = await llm_agent.negotiate(mock_phase_state)

    mock_llm_coordinator.call_json.assert_called_once()
    assert isinstance(messages, list)
    assert len(messages) == 1
    message = messages[0]
    assert isinstance(message, Message)
    assert message.recipient == "FRANCE"
    assert message.content == "Hello France"
    assert message.message_type == "private"


@pytest.mark.asyncio
async def test_decide_orders_returns_valid_orders(
    llm_agent, mock_llm_coordinator, mock_phase_state
):
    # Fixture mock_phase_state already provides units by default

    mock_llm_coordinator.call_json = AsyncMock(
        return_value={"orders": ["A LON H", "F EDI M NTH"]}
    )

    orders = await llm_agent.decide_orders(mock_phase_state)

    mock_llm_coordinator.call_json.assert_called_once()
    assert isinstance(orders, list)
    assert len(orders) == 2
    assert isinstance(orders[0], Order)
    assert orders[0].order_str == "A LON H"
    assert isinstance(orders[1], Order)
    assert orders[1].order_str == "F EDI M NTH"


@pytest.mark.asyncio
async def test_decide_orders_no_units(
    llm_agent, mock_llm_coordinator, mock_phase_state
):
    # Test case where the agent has no units
    mock_phase_state.get_power_units = Mock(
        return_value=[]
    )  # Override fixture for this test case

    orders = await llm_agent.decide_orders(mock_phase_state)

    # call_json should NOT be called if there are no units
    mock_llm_coordinator.call_json.assert_not_called()
    assert isinstance(orders, list)
    assert len(orders) == 0



================================================
File: test_lm_game.py
================================================
#!/usr/bin/env python3
"""
Modified version of lm_game.py for testing first round functionality.
This allows stepping through just the first round to validate API calls.
"""

import asyncio
import logging
import os
import time
from typing import List, Optional
from unittest.mock import patch
import pytest
import json

import dotenv
from diplomacy import Game
from diplomacy.utils.export import to_saved_game_format

# New refactored components
from ai_diplomacy.logging_setup import setup_logging
from ai_diplomacy.agent_manager import AgentManager
from ai_diplomacy.game_history import GameHistory
from ai_diplomacy.general_utils import (
    get_valid_orders,
    gather_possible_orders,
)
# Use the shared factory for GameConfig
from ._shared_fixtures import create_game_config
from ai_diplomacy.game_config import GameConfig


# Suppress warnings
os.environ["GRPC_PYTHON_LOG_LEVEL"] = "40"
os.environ["GRPC_VERBOSITY"] = "ERROR"
os.environ["ABSL_MIN_LOG_LEVEL"] = "2"
os.environ["GRPC_POLL_STRATEGY"] = "poll"

dotenv.load_dotenv()

logger = logging.getLogger(__name__)

# Default model for live tests - MAKE SURE THIS IS A VALID, ACCESSIBLE MODEL
LIVE_MODEL_ID = "gemma3:latest"  # Or use an environment variable


def _prepare_config_for_test(
    execution_mode: str,
    test_powers_str: str,
    model_ids_str: Optional[str] = None,
    num_players: int = 1,
    num_sequential: int = 2,
    max_concurrent: int = 2,
    log_to_file_override: bool = True,
    log_dir_override: str = "./pytest_logs"
) -> GameConfig:
    """Helper function to create GameConfig for tests using the shared factory."""

    use_mocks = execution_mode == "mock"

    actual_model_ids_str: str
    if use_mocks:
        actual_model_ids_str = (
            model_ids_str if model_ids_str else "mock_model_1,mock_model_2"
        )
    else:
        actual_model_ids_str = model_ids_str if model_ids_str else LIVE_MODEL_ID
        num_test_powers = len(test_powers_str.split(","))
        if "," not in actual_model_ids_str and num_test_powers > 1:
            actual_model_ids_str = ",".join([LIVE_MODEL_ID] * num_test_powers)

    parsed_model_ids = [m.strip() for m in actual_model_ids_str.split(",")]
    test_powers_list = [p.strip().upper() for p in test_powers_str.split(",")]
    
    num_models = len(parsed_model_ids)
    fixed_models = [ parsed_model_ids[i % num_models] if num_models > 0 else "default_mock_model" for i in range(len(test_powers_list)) ]


    config_kwargs = {
        "use_mocks": use_mocks,
        "dev_mode": True,
        "game_id_prefix": f"pytest_{execution_mode}",
        "log_level": "INFO",
        "log_to_file": log_to_file_override,
        "log_dir": log_dir_override,
        "test_powers": test_powers_str,
        "model_ids": parsed_model_ids,
        "fixed_models": fixed_models,
        "num_players": num_players,
        "power_name": None,
        "game_id": f"pytest_{execution_mode}_{int(time.time())}",
        "exclude_powers": None,
        "max_years": 1,
        "perform_planning_phase": False,
        "num_negotiation_rounds": 0,
        "negotiation_style": "simultaneous",
        "randomize_fixed_models": False,
        "models_config_file": None
    }
    
    config_kwargs["num_sequential"] = num_sequential
    config_kwargs["max_concurrent"] = max_concurrent


    config = create_game_config(**config_kwargs)
    setup_logging(config)
    return config


class GameTester:
    """Test class for stepping through game functionality."""

    def __init__(self, config: GameConfig):
        self.config = config
        self.game = None
        self.game_history = None
        self.agent_manager = None

    async def setup_game(self):
        """Set up the game environment."""
        logger.info(
            f"Setting up test game environment (Mocks: {self.config.args.use_mocks})..."
        )
        self.game = Game()
        self.game_history = GameHistory()
        self.agent_manager = AgentManager(self.config)
        logger.info(
            f"Game setup complete. Current phase: {self.game.current_short_phase}"
        )

    async def test_single_round(self, test_powers: List[str]):
        """Test a single round of order generation."""
        logger.info(
            f"Testing single round for powers: {test_powers} (Mocks: {self.config.args.use_mocks})"
        )
        powers_and_models = {}
        fixed_models_list = self.config.args.fixed_models
        for i, power in enumerate(test_powers):
            powers_and_models[power] = fixed_models_list[i]
        self.agent_manager.initialize_agents(powers_and_models)
        if not self.agent_manager.agents:
            logger.error("❌ No agents were initialized")
            return False
        logger.info(f"✅ Initialized {len(self.agent_manager.agents)} agents")
        success_count = 0
        total_count = len(test_powers)
        for power_name in test_powers:
            if power_name not in self.agent_manager.agents:
                logger.error(f"❌ Agent for {power_name} not found")
                continue
            logger.info(f"\n--- Testing {power_name} ---")
            try:
                success = await self.test_power_order_generation(power_name)
                if success:
                    success_count += 1
                    logger.info(f"✅ {power_name}: Order generation successful")
                else:
                    logger.error(f"❌ {power_name}: Order generation failed")
            except Exception as e:
                logger.error(
                    f"❌ {power_name}: Exception during test - {e}", exc_info=True
                )
        success_rate = (success_count / total_count) * 100 if total_count > 0 else 0
        logger.info(
            f"\n📊 Single round test results: {success_count}/{total_count} successful ({success_rate:.1f}%)"
        )
        return success_count == total_count

    async def _get_mocked_llm_call_internal(
        self, power_name_for_mock: str, *args, **kwargs
    ):
        """Helper to provide a mocked llm_call_internal behavior for a specific power."""
        mock_orders_db = {
            "FRANCE": ["A PAR H", "A MAR H", "F BRE H"],
            "GERMANY": ["A BER H", "A MUN H", "F KIE H"],
            "ENGLAND": ["F LON H", "F EDI H", "A LVP H"],
        }
        selected_orders = mock_orders_db.get(
            power_name_for_mock, [f"A {power_name_for_mock[:3].upper()} H"]
        )
        # Create a dictionary, then dump it to a JSON string.
        mock_orders_dict = {"orders": selected_orders}
        mock_response_json_string = json.dumps(mock_orders_dict)

        mock_full_response = f"Reasoning:\n- Mock reasoning for {power_name_for_mock}\n- These are test orders for validation\n\nPARSABLE OUTPUT:\n{mock_response_json_string}"
        return mock_full_response

    async def test_power_order_generation(self, power_name: str) -> bool:
        """Tests order generation for a single power."""
        agent = self.agent_manager.get_agent(power_name)
        if not agent:
            logger.error(
                f"Agent for {power_name} not found during order generation test."
            )
            return False
        current_phase = self.game.current_short_phase
        logger.info(
            f"[{power_name}] Current phase for order generation: {current_phase}"
        )
        board_state = self.game.get_state()
        possible_orders = gather_possible_orders(self.game, power_name)
        logger.info(f"Possible orders for {power_name}: {list(possible_orders.keys())}")
        log_file_path = os.path.join(
            self.config.game_id_specific_log_dir, "test_orders.csv"
        )
        os.makedirs(os.path.dirname(log_file_path), exist_ok=True)
        logger.info(f"🚀 Generating orders for {power_name}...")
        start_time = time.time()

        # Ensure agent.model_id, agent.system_prompt etc are correctly populated by AgentManager
        orders_callable = get_valid_orders(
            game=self.game,
            model_id=agent.model_id,
            agent_system_prompt=agent.system_prompt,
            board_state=board_state,
            power_name=power_name,
            possible_orders=possible_orders,
            game_history=self.game_history,
            game_id=self.config.game_id,
            config=self.config,
            agent_goals=agent.goals,
            agent_relationships=agent.relationships,
            agent_private_diary_str=agent.format_private_diary_for_prompt(),
            log_file_path=log_file_path,
            phase=current_phase,
        )
        orders = None
        try:
            if self.config.args.use_mocks:

                async def mock_side_effect(
                    *args_inner, **kwargs_inner
                ):  # Renamed args to avoid clash
                    return await self._get_mocked_llm_call_internal(
                        power_name, *args_inner, **kwargs_inner
                    )

                with patch(
                    "ai_diplomacy.services.llm_coordinator.llm_call_internal",
                    side_effect=mock_side_effect,
                ):
                    orders = await orders_callable
            else:
                orders = await orders_callable
        except LLMInvalidOutputError as e:
            logger.error(
                f"DEV_MODE: LLMInvalidOutputError for {power_name} ({agent.model_id}): {e}"
            )
            # Log details as before
            return False

        end_time = time.time()
        duration = end_time - start_time
        logger.info(f"⏱️  Order generation for {power_name} took {duration:.2f} seconds")
        logger.info(f"📋 Generated orders for {power_name}: {orders}")

        if self.config.args.use_mocks and orders is not None:
            mock_db = {
                "FRANCE": ["A PAR H", "A MAR H", "F BRE H"],
                "GERMANY": ["A BER H", "A MUN H", "F KIE H"],
                "ENGLAND": ["F LON H", "F EDI H", "A LVP H"],
            }
            expected_mocked_orders = mock_db.get(
                power_name, [f"A {power_name[:3].upper()} H"]
            )
            assert sorted(orders) == sorted(expected_mocked_orders), (
                f"Mock orders mismatch for {power_name}: expected {expected_mocked_orders}, got {orders}"
            )

        if orders and len(orders) > 0:
            logger.info(
                f"✅ Successfully generated {len(orders)} orders for {power_name}"
            )
            return True
        else:
            logger.warning(f"⚠️  No orders generated for {power_name}")
            return False

    async def test_sequential_calls(self, power_name: str, num_calls: int):
        """Test multiple sequential API calls."""
        logger.info(
            f"Testing {num_calls} sequential calls for {power_name} (Mocks: {self.config.args.use_mocks})"
        )

        # Agent initialization is now expected to be handled by the calling test function's setup
        # or test_single_round if that's what sets up agents.
        # For this specific test, we ensure the agent for power_name is initialized.
        if power_name not in self.agent_manager.agents:
            # Attempt to initialize just this one agent if not already present
            logger.info(
                f"Agent for {power_name} not found, attempting initialization for sequential test."
            )
            try:
                original_test_powers = [
                    p.strip().upper() for p in self.config.args.test_powers.split(",")
                ]
                power_index = original_test_powers.index(power_name)
                model_to_use_for_power = self.config.args.fixed_models[power_index]
                self.agent_manager.initialize_agents(
                    {power_name: model_to_use_for_power}
                )
            except (ValueError, IndexError) as e:
                logger.error(
                    f"❌ Failed to determine model for {power_name} for sequential test: {e}"
                )
                return False

        if power_name not in self.agent_manager.agents:
            logger.error(
                f"❌ Failed to initialize agent for {power_name} for sequential test."
            )
            return False

        success_count = 0
        for i in range(num_calls):
            logger.info(f"\n--- Sequential Call {i + 1}/{num_calls} ---")
            success = await self.test_power_order_generation(power_name)
            if success:
                success_count += 1
                logger.info(f"✅ Call {i + 1} for {power_name} succeeded")
            else:
                logger.warning(f"⚠️  Call {i + 1} for {power_name} failed")
            if i < num_calls - 1:
                await asyncio.sleep(0.1 if self.config.args.use_mocks else 1)
        success_rate = (success_count / num_calls) * 100
        logger.info(
            f"\n📊 Sequential test results for {power_name}: {success_count}/{num_calls} successful ({success_rate:.1f}%)"
        )
        return success_count == num_calls

    async def test_concurrent_calls(self, test_powers: List[str], max_concurrent: int):
        """Test concurrent API calls."""
        # Agent initialization is expected to be handled by the calling test function's setup.
        # This method will operate on the agents already initialized in self.agent_manager.

        concurrent_powers_to_test = [
            p for p in test_powers if p in self.agent_manager.agents
        ][:max_concurrent]
        if not concurrent_powers_to_test:
            logger.error("❌ No agents available or initialized for concurrent test.")
            return False

        logger.info(
            f"Testing concurrent calls for powers: {concurrent_powers_to_test} (Mocks: {self.config.args.use_mocks})"
        )

        tasks = [
            asyncio.create_task(self.test_power_order_generation(p_name))
            for p_name in concurrent_powers_to_test
        ]
        logger.info(f"🚀 Starting {len(tasks)} concurrent API calls...")
        start_time = time.time()
        results_list = await asyncio.gather(*tasks)
        end_time = time.time()
        duration = end_time - start_time
        results_map = {
            power_name: result
            for power_name, result in zip(concurrent_powers_to_test, results_list)
        }
        for p_name, res in results_map.items():
            logger.info(
                f"🏁 Concurrent result for {p_name}: {'Success' if res else 'Failed'}"
            )
        success_count = sum(1 for res in results_list if res)
        total_count = len(concurrent_powers_to_test)
        logger.info(
            f"\n📊 Concurrent test results: {success_count}/{total_count} successful in {duration:.2f}s"
        )
        return success_count == total_count


# --- Pytest Test Functions ---


@pytest.mark.asyncio
@pytest.mark.parametrize(
    "execution_mode", ["mock"]
)  # Temporarily disable live for focus
async def test_single_round_scenario(execution_mode, request: pytest.FixtureRequest):
    # if execution_mode == "live": # Integration marker removed as live mode is disabled here
    #     request.applymarker(pytest.mark.integration)

    test_powers_str = "FRANCE,GERMANY"
    # For live mode, ensure enough models are specified or use the default LIVE_MODEL_ID for all
    model_ids_str = (
        "mock_fr,mock_ge"
        if execution_mode == "mock"
        else f"{LIVE_MODEL_ID},{LIVE_MODEL_ID}"
    )

    config = _prepare_config_for_test(execution_mode, test_powers_str, model_ids_str)

    tester = GameTester(config)
    await (
        tester.setup_game()
    )  # Sets up agents based on config.args.test_powers and config.args.fixed_models

    # test_single_round expects a list of powers that are defined in config.args.test_powers
    # and have corresponding models in config.args.fixed_models
    # The setup_game initializes AgentManager, but test_single_round re-initializes agents
    # based on the test_powers list passed to it and models from config.args.fixed_models.
    # Ensure test_powers_list matches what fixed_models were set up for.
    test_powers_list = [p.strip().upper() for p in config.args.test_powers.split(",")]

    success = await tester.test_single_round(test_powers_list)
    assert success, f"Single round scenario failed in {execution_mode} mode."


@pytest.mark.asyncio
@pytest.mark.parametrize(
    "execution_mode", ["mock"]
)  # Temporarily disable live for focus
async def test_order_generation_scenario(
    execution_mode, request: pytest.FixtureRequest
):
    # if execution_mode == "live": # Integration marker removed as live mode is disabled here
    #     request.applymarker(pytest.mark.integration)

    power_to_test = "FRANCE"
    model_id_str = "mock_fr_single" if execution_mode == "mock" else LIVE_MODEL_ID
    config = _prepare_config_for_test(execution_mode, power_to_test, model_id_str)

    tester = GameTester(config)
    await tester.setup_game()

    # Initialize agent for the single power to test
    # test_power_order_generation expects agent to be in agent_manager
    assert isinstance(
        tester.agent_manager, AgentManager
    )  # Ensure agent_manager is AgentManager
    tester.agent_manager.initialize_agents({power_to_test: config.args.fixed_models[0]})

    success = await tester.test_power_order_generation(power_to_test)
    assert success, (
        f"Order generation scenario for {power_to_test} failed in {execution_mode} mode."
    )


@pytest.mark.asyncio
@pytest.mark.parametrize(
    "execution_mode", ["mock"]
)  # Temporarily disable live for focus
async def test_sequential_calls_scenario(
    execution_mode, request: pytest.FixtureRequest
):
    # if execution_mode == "live": # Integration marker removed as live mode is disabled here
    #     request.applymarker(pytest.mark.integration)

    power_to_test = "FRANCE"
    num_sequential_calls = 2  # Reduced for test speed
    model_id_str = "mock_fr_seq" if execution_mode == "mock" else LIVE_MODEL_ID
    config = _prepare_config_for_test(
        execution_mode, power_to_test, model_id_str, num_sequential=num_sequential_calls
    )

    tester = GameTester(config)
    await tester.setup_game()

    # Agent for power_to_test needs to be initialized.
    # The test_sequential_calls method itself re-initializes the agent.
    # So, ensuring config.args.test_powers and config.args.fixed_models are correctly set up by _prepare_config_for_test is key.

    success = await tester.test_sequential_calls(power_to_test, num_sequential_calls)
    assert success, (
        f"Sequential calls scenario for {power_to_test} failed in {execution_mode} mode."
    )


@pytest.mark.asyncio
@pytest.mark.parametrize(
    "execution_mode", ["mock"]
)  # Temporarily disable live for focus
async def test_concurrent_calls_scenario(
    execution_mode, request: pytest.FixtureRequest
):
    # if execution_mode == "live": # Integration marker removed as live mode is disabled here
    #     request.applymarker(pytest.mark.integration)

    test_powers_str = "FRANCE,GERMANY"
    max_concurrent_calls = 2
    # For live mode, ensure enough models are specified or use the default LIVE_MODEL_ID for all
    model_ids_str = (
        "mock_fr_con,mock_ge_con"
        if execution_mode == "mock"
        else f"{LIVE_MODEL_ID},{LIVE_MODEL_ID}"
    )

    config = _prepare_config_for_test(
        execution_mode,
        test_powers_str,
        model_ids_str,
        max_concurrent=max_concurrent_calls,
    )

    tester = GameTester(config)
    await tester.setup_game()

    # test_concurrent_calls expects agents to be initialized.
    # It will select from agents already in tester.agent_manager.agents.
    # The AgentManager is initialized in setup_game using config.args.test_powers and config.args.fixed_models.
    # We need to ensure the agents for test_powers_str are initialized.

    powers_to_test_list = [p.strip().upper() for p in test_powers_str.split(",")]

    # Initialize agents that will be used in the concurrent test
    powers_and_models_for_concurrent = {}
    for i, power_name in enumerate(powers_to_test_list):
        powers_and_models_for_concurrent[power_name] = config.args.fixed_models[i]
    assert isinstance(
        tester.agent_manager, AgentManager
    )  # Ensure agent_manager is AgentManager
    tester.agent_manager.initialize_agents(powers_and_models_for_concurrent)

    success = await tester.test_concurrent_calls(
        powers_to_test_list, max_concurrent_calls
    )
    assert success, f"Concurrent calls scenario failed in {execution_mode} mode."



================================================
File: test_mocked_llm_calls.py
================================================
import asyncio
import pytest
from unittest.mock import patch, MagicMock

# AgentLLMInterface was removed in refactor
from ai_diplomacy.game_config import GameConfig
from diplomacy import Game
from ai_diplomacy.game_history import GameHistory
from ai_diplomacy.agents.base import BaseAgent
from ai_diplomacy.agents.llm_agent import LLMAgent
from ai_diplomacy.general_utils import (
    gather_possible_orders,
    get_valid_orders,
)  # Step 1: Import get_valid_orders and gather_possible_orders


# Helper class for configuration, can be defined at module level
class TestArgs:
    __test__ = False  # Prevent pytest from collecting this as a test class

    def __init__(self):
        self.power_name = "FRANCE"
        self.model_ids = ["test_model"]
        self.num_players = 1
        self.game_id_prefix = "test_mock"
        self.game_id = "test_mock_123"
        self.log_level = "INFO"
        self.log_to_file = False
        self.log_dir = "./mock_test_logs"
        self.perform_planning_phase = False
        self.num_negotiation_rounds = 0
        self.negotiation_style = "simultaneous"
        self.fixed_models = ["test_model"]
        self.randomize_fixed_models = False
        self.exclude_powers = None
        self.max_years = None
        self.test_powers = "FRANCE"


@pytest.fixture
def common_mocks():
    args = TestArgs()
    config = GameConfig(args)  # type: ignore

    game = Game()
    # game.phase = "S1901M" # Set phase directly - this is usually handled by game processing
    # For tests, it's better to ensure the game object is in the expected state.
    # If get_all_possible_orders or other methods depend on phase, ensure it's set.
    # However, diplomacy.Game() starts in S1901M by default.

    game_history = MagicMock(spec=GameHistory)

    # game_history.get_event_log_for_power.return_value = "Fake history log"
    # game_history.get_full_event_log.return_value = "Fake full history log"
    game_history.get_messages_this_round.return_value = "Fake messages for this round"

    # agent_interface is None in the original setup for these tests
    return {
        "game": game,
        "game_history": game_history,
        "config": config,
        "power_name": "FRANCE",  # Default power for these tests
    }


@pytest.mark.asyncio
@patch("ai_diplomacy.services.llm_coordinator.llm_call_internal")
async def test_mock_get_valid_orders_success(mock_llm_call_internal, common_mocks):
    game = common_mocks["game"]
    game_history = common_mocks["game_history"]
    config = common_mocks["config"]
    power_name = common_mocks["power_name"]

    # Units for FRANCE in S1901M: A PAR, A MAR, F BRE
    game.set_units("FRANCE", ["A PAR", "A MAR", "F BRE"])
    # Ensure game phase is S1901M for get_all_possible_orders to be accurate
    # game.phase = "S1901M" # diplomacy.Game() starts in S1901M

    # Use power-specific possible orders
    power_specific_possible_orders = gather_possible_orders(game, power_name)

    mock_response_json_string = '{"orders": ["A PAR H", "A MAR H", "F BRE H"]}'
    # llm_call_internal is async, so its mock should be awaitable or return an awaitable
    # Patch creates a MagicMock. If its return_value is set to a future, it works.
    # Or, if return_value is a direct value, it should be fine if the mock is treated as awaitable.
    # For clarity with async functions, AsyncMock is often preferred, but MagicMock can work.
    fut = asyncio.Future()
    fut.set_result(mock_response_json_string)
    mock_llm_call_internal.return_value = fut

    orders = await get_valid_orders(
        game=game,
        model_id="test_model",
        agent_system_prompt="System prompt for FRANCE",
        board_state=game.get_state(),  # board_state is phase dependent
        power_name=power_name,
        possible_orders=power_specific_possible_orders,  # Use power-specific
        game_history=game_history,
        game_id=config.game_id,
        config=config,
        agent_goals=["Goal 1"],
        agent_relationships={"GERMANY": "Neutral"},
        log_file_path="./mock_test_logs/orders.csv",  # Consider using tmp_path fixture for logs
        phase=game.phase,  # Use game.phase
    )

    mock_llm_call_internal.assert_called_once()
    # Sort both lists for comparison as order is not guaranteed
    assert sorted(orders) == sorted(["A PAR H", "A MAR H", "F BRE H"])


@pytest.mark.asyncio
@patch("ai_diplomacy.services.llm_coordinator.llm_call_internal")
async def test_mock_get_valid_orders_json_fail(mock_llm_call_internal, common_mocks):
    game = common_mocks["game"]
    game_history = common_mocks["game_history"]
    config = common_mocks["config"]
    power_name = common_mocks["power_name"]

    game.set_units("FRANCE", ["A PAR", "A MAR", "F BRE"])
    # game.phase = "S1901M"

    # Use power-specific possible orders
    power_specific_possible_orders = gather_possible_orders(game, power_name)

    # expected_fallback_orders should be calculated based on these power_specific_possible_orders
    expected_fallback_orders = []
    for loc_str_key in power_specific_possible_orders:  # loc_str_key is like "A PAR"
        orders_for_loc = power_specific_possible_orders[loc_str_key]
        # unit_type = loc_str_key.split()[0] # Not needed if using holds[0] or orders_for_loc[0]
        # unit_loc_name = loc_str_key.split()[1]
        hold_order_candidates = [o for o in orders_for_loc if o.endswith(" H")]

        if hold_order_candidates:
            expected_fallback_orders.append(hold_order_candidates[0])
        elif orders_for_loc:  # If no hold, take the first available order
            expected_fallback_orders.append(orders_for_loc[0])
    expected_fallback_orders.sort()

    mock_response_malformed_json_string = '{"orders": ["A PAR H", "A MAR H", "F BRE H"'
    fut = asyncio.Future()
    fut.set_result(mock_response_malformed_json_string)
    mock_llm_call_internal.return_value = fut

    orders = await get_valid_orders(
        game=game,
        model_id="test_model",
        agent_system_prompt="System prompt",
        board_state=game.get_state(),
        power_name=power_name,
        possible_orders=power_specific_possible_orders,  # Use power-specific
        game_history=game_history,
        game_id=config.game_id,
        config=config,
        phase=game.phase,
    )

    mock_llm_call_internal.assert_called_once()
    orders.sort()
    assert orders == expected_fallback_orders


@pytest.mark.asyncio
@patch("ai_diplomacy.services.llm_coordinator.llm_call_internal")
async def test_mock_get_valid_orders_empty_response(
    mock_llm_call_internal, common_mocks
):
    game = common_mocks["game"]
    game_history = common_mocks["game_history"]
    config = common_mocks["config"]
    power_name = common_mocks["power_name"]

    game.set_units("FRANCE", ["A PAR", "A MAR", "F BRE"])
    # game.phase = "S1901M"

    # Use power-specific possible orders
    power_specific_possible_orders = gather_possible_orders(game, power_name)

    # expected_fallback_orders should be calculated based on these power_specific_possible_orders
    expected_fallback_orders = []
    for loc_str_key in power_specific_possible_orders:  # loc_str_key is like "A PAR"
        orders_for_loc = power_specific_possible_orders[loc_str_key]
        hold_order_candidates = [o for o in orders_for_loc if o.endswith(" H")]

        if hold_order_candidates:
            expected_fallback_orders.append(hold_order_candidates[0])
        elif orders_for_loc:  # If no hold, take the first available order
            expected_fallback_orders.append(orders_for_loc[0])
    expected_fallback_orders.sort()

    mock_empty_response_string = ""
    fut = asyncio.Future()
    fut.set_result(mock_empty_response_string)
    mock_llm_call_internal.return_value = fut

    orders = await get_valid_orders(
        game=game,
        model_id="test_model",
        agent_system_prompt="System prompt",
        board_state=game.get_state(),
        power_name=power_name,
        possible_orders=power_specific_possible_orders,  # Use power-specific
        game_history=game_history,
        game_id=config.game_id,
        config=config,
        phase=game.phase,
    )

    mock_llm_call_internal.assert_called_once()
    orders.sort()
    assert orders == expected_fallback_orders





================================================
File: test_model_integration.py
================================================
import pytest
import llm

# AgentLLMInterface was removed in refactor
from ai_diplomacy.services.llm_coordinator import LLMCoordinator

MODEL_ID = "gemma3:latest"  # Or your specific model


@pytest.mark.integration
@pytest.mark.skipif(
    MODEL_ID not in llm.get_model_aliases(),
    reason=f"Local model {MODEL_ID} not installed or not found by llm library",
)
def test_get_model_and_prompt():
    """
    Tests if the specified LLM model can be loaded and can respond to a simple prompt.
    """
    _fail = pytest.fail  # Local alias for pytest.fail
    try:
        model = llm.get_model(MODEL_ID)
    except llm.UnknownModelError as e:
        _fail(
            f"Failed to get model '{MODEL_ID}'. Is it installed and configured correctly? Error: {e}"
        )
    except Exception as e:
        _fail(f"An unexpected error occurred while getting the model: {e}")

    assert model is not None, f"Model '{MODEL_ID}' could not be loaded."

    # Test a simple synchronous prompt
    try:
        response = model.prompt("Say 'test'")
        assert response is not None, "Model returned a None response."
        assert response.text().strip().lower() == "test", (
            f"Model did not respond as expected. Response: '{response.text()}'"
        )
        print(
            f"Synchronous prompt successful with {MODEL_ID}. Response: {response.text()}"
        )
    except AttributeError as e:
        if "async_prompt" in str(e) or "prompt" not in dir(model):
            _fail(
                f"The model object for '{MODEL_ID}' does not have a `prompt` method. Available methods: {dir(model)}. Error: {e}"
            )
        else:
            _fail(f"AttributeError during synchronous prompt: {e}")
    except Exception as e:
        _fail(f"An error occurred during synchronous prompt with '{MODEL_ID}': {e}")

    # Test a simple asynchronous prompt if the synchronous one worked
    # Note: We'll need an event loop to run this part of the test if it's not already managed by pytest-asyncio
    # For now, let's assume the main issue is with model loading or the basic prompt method.
    # If `model.prompt` works, `await model.prompt(...)` should also work in an async context.


class DummyCoordinator(LLMCoordinator):
    async def request(
        self,
        model_id,
        prompt_text,
        system_prompt_text,
        game_id="test_game",
        agent_name="test_agent",
        phase_str="test_phase",
        request_identifier="request",
    ):
        # Simulate a successful LLM response
        return "This is a dummy LLM response."



================================================
File: test_stage0.py
================================================
#!/usr/bin/env python3
"""
Test script for Stage 0 of the refactor.
Verifies that the new directory structure and basic components work.
"""

import logging
import pytest  # Add pytest
from ai_diplomacy.core.state import PhaseState

from ai_diplomacy.agents.scripted_agent import ScriptedAgent
from ai_diplomacy.services.config import DiplomacyConfig, AgentConfig, GameConfig
from ai_diplomacy.services.llm_coordinator import LLMCoordinator

# Set up logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


def test_core_state():
    """Test PhaseState creation."""
    logger.info("Testing PhaseState creation...")

    # Create a minimal test phase state
    phase = PhaseState(
        phase_name="S1901M",
        year=1901,
        season="SPRING",
        phase_type="MOVEMENT",
        powers=frozenset(["FRANCE", "GERMANY"]),
        units={"FRANCE": ["A PAR", "F BRE"], "GERMANY": ["A BER", "F KIE"]},
        supply_centers={
            "FRANCE": ["PAR", "BRE", "MAR"],
            "GERMANY": ["BER", "KIE", "MUN"],
        },
    )

    assert phase.get_center_count("FRANCE") == 3
    assert phase.get_center_count("GERMANY") == 3
    assert not phase.is_power_eliminated("FRANCE")

    logger.info("✓ PhaseState working correctly")


@pytest.mark.asyncio
async def test_scripted_agent():
    """Test scripted agent functionality."""
    logger.info("Testing ScriptedAgent...")

    agent = ScriptedAgent("test-france", "FRANCE", "neutral")

    # Create test phase state
    phase = PhaseState(
        phase_name="S1901M",
        year=1901,
        season="SPRING",
        phase_type="MOVEMENT",
        powers=frozenset(["FRANCE", "GERMANY"]),
        units={"FRANCE": ["A PAR", "F BRE"], "GERMANY": ["A BER", "F KIE"]},
        supply_centers={
            "FRANCE": ["PAR", "BRE", "MAR"],
            "GERMANY": ["BER", "KIE", "MUN"],
        },
    )

    # Test order generation
    orders = await agent.decide_orders(phase)
    logger.info(f"Generated {len(orders)} orders: {[str(o) for o in orders]}")

    # Test message generation
    messages = await agent.negotiate(phase)
    logger.info(f"Generated {len(messages)} messages")

    # Test state update
    await agent.update_state(phase, [])

    logger.info("✓ ScriptedAgent working correctly")


def test_config():
    """Test configuration system."""
    logger.info("Testing configuration system...")

    # Test creating config from scratch
    game_config = GameConfig(token_budget=5000, use_mcp=False)
    agent_config = AgentConfig(country="FRANCE", type="llm", model_id="gpt-4o-mini")
    config = DiplomacyConfig(game=game_config, agents=[agent_config])

    assert config.game.token_budget == 5000
    agent_config = config.get_agent_config("FRANCE")
    assert agent_config is not None
    assert agent_config.model_id == "gpt-4o-mini"
    assert len(config.get_llm_agents()) == 1

    logger.info("✓ Configuration system working correctly")


def test_llm_coordinator():
    """Test LLM coordinator initialization."""
    logger.info("Testing LLM coordinator...")

    # Just test initialization for now
    coordinator = LLMCoordinator()
    assert coordinator is not None

    logger.info("✓ LLM coordinator initialized correctly")



================================================
File: test_stage1.py
================================================
#!/usr/bin/env python3
"""
Test script for Stage 1 of the refactor.
Verifies that the clean agent boundary works correctly.
"""

import logging
from ai_diplomacy.core.state import PhaseState
from ai_diplomacy.core.manager import GameEvent
from ai_diplomacy.agents.factory import AgentFactory
from ai_diplomacy.agents.llm_agent import LLMAgent
from ai_diplomacy.services.config import DiplomacyConfig, AgentConfig, GameConfig

# Set up logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


def test_agent_factory():
    """Test the agent factory creation."""
    logger.info("Testing AgentFactory...")

    factory = AgentFactory()

    # Test LLM agent creation
    llm_config = AgentConfig(country="FRANCE", type="llm", model_id="gpt-4o-mini")
    llm_agent = factory.create_agent("test-llm", "FRANCE", llm_config, "test-game")

    assert isinstance(llm_agent, LLMAgent)
    assert llm_agent.country == "FRANCE"
    assert llm_agent.config.model_id == "gpt-4o-mini"

    # Test scripted agent creation
    scripted_config = AgentConfig(country="GERMANY", type="scripted")
    scripted_agent = factory.create_agent(
        "test-scripted", "GERMANY", scripted_config, "test-game"
    )

    assert scripted_agent.country == "GERMANY"
    assert scripted_agent.get_agent_info()["type"] == "ScriptedAgent"

    logger.info("✓ AgentFactory working correctly")


def test_config_integration():
    """Test creating agents from full configuration."""
    logger.info("Testing configuration integration...")

    # Create a diplomacy configuration
    game_config = GameConfig(token_budget=5000, use_mcp=False)
    agents_config = [
        AgentConfig(country="FRANCE", type="llm", model_id="gpt-4o-mini"),
        AgentConfig(country="GERMANY", type="scripted"),
        AgentConfig(country="ENGLAND", type="llm", model_id="claude-3-haiku"),
    ]
    config = DiplomacyConfig(game=game_config, agents=agents_config)

    # Create agents from config
    factory = AgentFactory()
    agents = factory.create_agents_from_config(config, "test-game")

    assert len(agents) == 3
    assert "FRANCE" in agents
    assert "GERMANY" in agents
    assert "ENGLAND" in agents

    # Verify agent types
    assert isinstance(agents["FRANCE"], LLMAgent)
    assert agents["GERMANY"].get_agent_info()["type"] == "ScriptedAgent"
    assert isinstance(agents["ENGLAND"], LLMAgent)

    logger.info("✓ Configuration integration working correctly")


def test_game_manager():
    """Test the core game manager."""
    logger.info("Testing GameManager...")

    # We need a mock diplomacy Game for testing
    # For now, let's test what we can without the actual game

    # Test GameEvent creation
    event = GameEvent(
        event_type="unit_lost",
        phase="S1901M",
        participants={"country": "FRANCE", "unit": "A PAR"},
        details={"unit_type": "A"},
    )

    assert event.event_type == "unit_lost"
    assert event.participants["country"] == "FRANCE"

    logger.info("✓ GameManager components working correctly")


def test_clean_boundaries():
    """Test that the clean boundaries are maintained."""
    logger.info("Testing clean boundaries...")

    # Test that PhaseState is immutable
    phase = PhaseState(
        phase_name="S1901M",
        year=1901,
        season="SPRING",
        phase_type="MOVEMENT",
        powers=frozenset(["FRANCE"]),
        units={"FRANCE": ["A PAR"]},
        supply_centers={"FRANCE": ["PAR"]},
    )

    # Try to modify it (should not work due to frozen=True)
    try:
        phase.year = 1902  # This should fail
        assert False, "PhaseState should be immutable"
    except Exception:
        pass  # Expected

    # Test that agents receive PhaseState, not Game objects
    config = AgentConfig(country="FRANCE", type="llm", model_id="gpt-4o-mini")
    agent = LLMAgent("test", "FRANCE", config)

    # Verify agent doesn't have direct game access
    assert not hasattr(agent, "game")
    assert hasattr(agent, "config")
    assert hasattr(agent, "llm_coordinator")

    logger.info("✓ Clean boundaries maintained")



================================================
File: test_stage2.py
================================================
#!/usr/bin/env python3
"""
Test script for Stage 2 of the refactor.
Verifies that the pluggable context provider system works correctly.
"""

import logging
import pytest  # Add pytest
from unittest.mock import patch
from ai_diplomacy.core.state import PhaseState
from ai_diplomacy.agents.factory import AgentFactory
from ai_diplomacy.agents.llm_agent import LLMAgent
from ai_diplomacy.agents.base import Order  # Import Order
from ai_diplomacy.services.config import DiplomacyConfig, AgentConfig, GameConfig
from ai_diplomacy.services.context_provider import (
    ContextProviderFactory,
    InlineContextProvider,
    MCPContextProvider,
    ContextData,
)

# Set up logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


def test_context_provider_factory():
    """Test the context provider factory."""
    logger.info("Testing ContextProviderFactory...")

    factory = ContextProviderFactory()

    # Test inline provider
    inline_provider = factory.get_provider("inline")
    assert isinstance(inline_provider, InlineContextProvider)
    assert inline_provider.is_available()

    # Test MCP provider (should fallback to inline since MCP client not configured)
    mcp_provider_fallback = factory.get_provider("mcp")
    assert isinstance(mcp_provider_fallback, InlineContextProvider)  # Should fallback

    # Test auto provider (should return inline since MCP not available)
    auto_provider = factory.get_provider("auto")
    assert isinstance(auto_provider, InlineContextProvider)

    # Test available providers
    available = factory.get_available_providers()
    assert "inline" in available

    logger.info("✓ ContextProviderFactory working correctly")


@pytest.mark.asyncio
async def test_inline_context_provider():
    """Test the inline context provider."""
    logger.info("Testing InlineContextProvider...")

    provider = InlineContextProvider()

    # Create test phase state
    phase_state = PhaseState(
        phase_name="S1901M",
        year=1901,
        season="SPRING",
        phase_type="MOVEMENT",
        powers=frozenset(["FRANCE", "GERMANY"]),
        units={"FRANCE": ["A PAR", "F BRE"], "GERMANY": ["A BER", "F KIE"]},
        supply_centers={
            "FRANCE": ["PAR", "BRE", "MAR"],
            "GERMANY": ["BER", "KIE", "MUN"],
        },
    )

    # Create test context data
    context_data = ContextData(
        phase_state=phase_state,
        possible_orders={
            "A PAR": ["A PAR H", "A PAR-BUR"],
            "F BRE": ["F BRE H", "F BRE-ENG"],
        },
        recent_messages="France to Germany: Hello!",
        strategic_analysis="Paris is well defended.",
    )

    # Create test agent config
    agent_config = AgentConfig(country="FRANCE", type="llm", model_id="gpt-4o-mini")

    # Test context provision
    result = await provider.provide_context(
        "test-agent", "FRANCE", context_data, agent_config
    )

    assert result["provider_type"] == "inline"
    assert "context_text" in result
    assert result["tools_available"] is False

    # Check that context contains expected information
    context_text = result["context_text"]
    assert "FRANCE" in context_text
    assert "S1901M" in context_text
    assert "A PAR" in context_text
    assert "F BRE" in context_text
    assert "Hello!" in context_text

    logger.info("✓ InlineContextProvider working correctly")


@pytest.mark.asyncio
async def test_mcp_context_provider():
    """Test the MCP context provider (should show tools are not available)."""
    logger.info("Testing MCPContextProvider...")

    provider = MCPContextProvider()

    # Create test data (same as inline test)
    phase_state = PhaseState(
        phase_name="S1901M",
        year=1901,
        season="SPRING",
        phase_type="MOVEMENT",
        powers=frozenset(["FRANCE", "GERMANY"]),
        units={"FRANCE": ["A PAR", "F BRE"], "GERMANY": ["A BER", "F KIE"]},
        supply_centers={
            "FRANCE": ["PAR", "BRE", "MAR"],
            "GERMANY": ["BER", "KIE", "MUN"],
        },
    )

    context_data = ContextData(
        phase_state=phase_state,
        possible_orders={"A PAR": ["A PAR H", "A PAR-BUR"]},
        recent_messages="Test message",
    )

    agent_config = AgentConfig(country="FRANCE", type="llm", model_id="gpt-4o-mini")

    # Test context provision (should show tools not available)
    result = await provider.provide_context(
        "test-agent", "FRANCE", context_data, agent_config
    )

    assert result["provider_type"] == "mcp"
    assert result["tools_available"] is False
    assert "MCP tools not available" in result["context_text"]

    logger.info("✓ MCPContextProvider correctly shows tools not available")


def test_config_context_provider():
    """Test that agent configs specify context providers correctly."""
    logger.info("Testing context provider configuration...")

    # Test explicit inline config
    inline_config = AgentConfig(
        country="FRANCE", type="llm", model_id="gpt-4o-mini", context_provider="inline"
    )
    assert inline_config.context_provider == "inline"

    # Test explicit MCP config
    mcp_config = AgentConfig(
        country="GERMANY", type="llm", model_id="claude-3-haiku", context_provider="mcp"
    )
    assert mcp_config.context_provider == "mcp"

    # Test auto config (default)
    auto_config = AgentConfig(country="ENGLAND", type="llm", model_id="gpt-4o")
    assert auto_config.context_provider == "auto"

    # Test resolve_context_provider function
    from ai_diplomacy.services.config import resolve_context_provider

    # Tool-capable model should resolve to MCP
    tool_config = AgentConfig(
        country="RUSSIA", type="llm", model_id="gpt-4o", context_provider="auto"
    )
    resolved = resolve_context_provider(tool_config)
    assert resolved == "mcp"

    # Non-tool model should resolve to inline
    simple_config = AgentConfig(
        country="ITALY", type="llm", model_id="ollama/llama3", context_provider="auto"
    )
    resolved = resolve_context_provider(simple_config)
    assert resolved == "inline"

    logger.info("✓ Context provider configuration working correctly")


@pytest.mark.asyncio
async def test_agent_with_context_providers():
    """Test that agents work correctly with different context providers."""
    logger.info("Testing agents with context providers...")

    # Create agents with different context provider configs
    inline_config = AgentConfig(
        country="FRANCE", type="llm", model_id="gpt-4o-mini", context_provider="inline"
    )
    mcp_config = AgentConfig(
        country="GERMANY", type="llm", model_id="gpt-4o", context_provider="mcp"
    )
    auto_config = AgentConfig(
        country="ENGLAND",
        type="llm",
        model_id="claude-3-haiku",
        context_provider="auto",
    )

    factory = AgentFactory()

    # Create agents
    inline_agent = factory.create_agent(
        "inline-test", "FRANCE", inline_config, "test-game"
    )
    mcp_agent = factory.create_agent("mcp-test", "GERMANY", mcp_config, "test-game")
    auto_agent = factory.create_agent("auto-test", "ENGLAND", auto_config, "test-game")

    # Check that agents have correct context providers
    assert isinstance(inline_agent, LLMAgent)
    assert inline_agent.resolved_context_provider_type == "inline"

    assert isinstance(mcp_agent, LLMAgent)
    assert (
        mcp_agent.resolved_context_provider_type == "inline"
    )  # Should fallback since MCP not available

    assert isinstance(auto_agent, LLMAgent)
    assert (
        auto_agent.resolved_context_provider_type == "inline"
    )  # Should fallback since MCP not available

    # Create test phase state
    phase_state = PhaseState(
        phase_name="S1901M",
        year=1901,
        season="SPRING",
        phase_type="MOVEMENT",
        powers=frozenset(["FRANCE", "GERMANY", "ENGLAND"]),
        units={"FRANCE": ["A PAR"], "GERMANY": ["A BER"], "ENGLAND": ["F LON"]},
        supply_centers={"FRANCE": ["PAR"], "GERMANY": ["BER"], "ENGLAND": ["LON"]},
    )

    # Test that agents can call decide_orders with context providers
    mock_llm_orders_string_output = '{"orders": ["A PAR H"]}'
    expected_agent_orders = [Order("A PAR H")]

    # Test inline_agent
    with patch(
        "ai_diplomacy.services.llm_coordinator.llm_call_internal",
        return_value=mock_llm_orders_string_output,
    ) as mock_llm_call:
        orders = await inline_agent.decide_orders(phase_state)
        assert orders == expected_agent_orders
        mock_llm_call.assert_called_once()
        # Verify context from inline provider was used in the prompt
        prompt_text = mock_llm_call.call_args[1]["prompt_text"]
        assert "Context (Inline)" in prompt_text

    # Test mcp_agent
    with patch(
        "ai_diplomacy.services.llm_coordinator.llm_call_internal",
        return_value=mock_llm_orders_string_output,
    ) as mock_llm_call:
        orders = await mcp_agent.decide_orders(phase_state)
        assert orders == expected_agent_orders
        mock_llm_call.assert_called_once()
        # Verify context from MCP provider (fallback to inline) was used
        prompt_text = mock_llm_call.call_args[1]["prompt_text"]
        assert "Context (MCP - Fallback to Inline)" in prompt_text
        assert "MCP tools not available" in prompt_text

    # Test auto_agent
    with patch(
        "ai_diplomacy.services.llm_coordinator.llm_call_internal",
        return_value=mock_llm_orders_string_output,
    ) as mock_llm_call:
        orders = await auto_agent.decide_orders(phase_state)
        assert orders == expected_agent_orders
        mock_llm_call.assert_called_once()
        # Verify context from auto provider (fallback to inline) was used
        prompt_text = mock_llm_call.call_args[1]["prompt_text"]
        assert "Context (Auto - Fallback to Inline)" in prompt_text

    logger.info("✓ Agents working correctly with context providers")


def test_full_config_integration():
    """Test creating agents from full configuration with context providers."""
    logger.info("Testing full configuration integration with context providers...")

    # Create a full diplomacy configuration with mixed context providers
    game_config = GameConfig(token_budget=5000, use_mcp=False)
    agents_config = [
        AgentConfig(
            country="FRANCE",
            type="llm",
            model_id="gpt-4o-mini",
            context_provider="inline",
        ),
        AgentConfig(
            country="GERMANY", type="llm", model_id="gpt-4o", context_provider="mcp"
        ),
        AgentConfig(
            country="ENGLAND",
            type="llm",
            model_id="claude-3-haiku",
            context_provider="auto",
        ),
        AgentConfig(country="RUSSIA", type="scripted"),
    ]
    config = DiplomacyConfig(game=game_config, agents=agents_config)

    # Create agents from config
    factory = AgentFactory()
    agents = factory.create_agents_from_config(config, "test-game")

    assert len(agents) == 4
    assert isinstance(agents["FRANCE"], LLMAgent)
    assert agents["FRANCE"].resolved_context_provider_type == "inline"

    assert isinstance(agents["GERMANY"], LLMAgent)
    assert (
        agents["GERMANY"].resolved_context_provider_type == "inline"
    )  # Fallback

    assert isinstance(agents["ENGLAND"], LLMAgent)
    assert (
        agents["ENGLAND"].resolved_context_provider_type == "inline"
    )  # Fallback for non-tool model

    # Scripted agent doesn't have context providers
    assert agents["RUSSIA"].get_agent_info()["type"] == "ScriptedAgent"
    assert not hasattr(agents["RUSSIA"], "resolved_context_provider_type")

    logger.info("✓ Full configuration integration with context providers working correctly")



================================================
File: unit/__init__.py
================================================



================================================
File: unit/test_phase_parsing.py
================================================
import pytest
from unittest.mock import MagicMock
from ai_diplomacy.utils.phase_parsing import get_phase_type_from_game, _extract_year_from_phase, PhaseType

# Test cases for get_phase_type_from_game
@pytest.mark.parametrize(
    "phase_string, expected_type",
    [
        ("SPRING 1901 MOVEMENT", PhaseType.MVT.value),
        ("S1901M", PhaseType.MVT.value),
        ("FALL 1902 RETREAT", PhaseType.RET.value),
        ("F1902R", PhaseType.RET.value),
        ("WINTER 1903 ADJUSTMENTS", PhaseType.BLD.value),
        ("W1903A", PhaseType.BLD.value),
        ("winter 1903 adjustments", PhaseType.BLD.value), # Lowercase
        ("Spring 1901 Movement", PhaseType.MVT.value), # Mixed case
        ("FORMING", "-"),
        ("COMPLETED", "-"),
        ("S1901MVT", PhaseType.MVT.value), # another variation of MVT
        ("AUTUMN 1901 BUILD", PhaseType.BLD.value), # another variation of BLD/Adjustments
        ("A1901B", PhaseType.BLD.value), # variation
        # Cases from test_game_orchestrator.py
        ("FALL 1901 RETREAT", PhaseType.RET.value),
        ("WINTER 1901 ADJUSTMENT", PhaseType.BLD.value),
        ("WINTER 1901 BUILD", PhaseType.BLD.value),
        ("AUTUMN 1905 ADJUSTMENTS", PhaseType.BLD.value),
        ("S1902M", PhaseType.MVT.value),
        ("F1903 RET", PhaseType.RET.value),
        ("WINTER 1904 BLD", PhaseType.BLD.value),
        ("SPR 1901 M", PhaseType.MVT.value),
        ("FAL 1901 R", PhaseType.RET.value),
        ("WIN 1901 A", PhaseType.BLD.value),
    ],
)
def test_get_phase_type_from_game_valid(phase_string, expected_type):
    mock_game = MagicMock()
    mock_game.get_current_phase.return_value = phase_string
    assert get_phase_type_from_game(mock_game) == expected_type

@pytest.mark.parametrize(
    "invalid_phase_string",
    [
        ("X1901Z"),
        ("SUMMER 1904 PICNIC"),
        ("S190B"), # Invalid compact form
        ("SPRANG 1901 MOVEMENT"), # Misspelled season
        ("XYZ1234 UNKNOWN_PHASE"), # Case from test_game_orchestrator.py
    ]
)
def test_get_phase_type_from_game_invalid(invalid_phase_string):
    mock_game = MagicMock()
    mock_game.get_current_phase.return_value = invalid_phase_string
    with pytest.raises(RuntimeError):
        get_phase_type_from_game(mock_game)

def test_get_phase_type_from_game_empty_phase():
    mock_game = MagicMock()
    mock_game.get_current_phase.return_value = ""
    assert get_phase_type_from_game(mock_game) == "-"
    mock_game.get_current_phase.return_value = None
    assert get_phase_type_from_game(mock_game) == "-"

# Test cases for _extract_year_from_phase
@pytest.mark.parametrize(
    "phase_string, expected_year",
    [
        ("SPRING 1901 MOVEMENT", 1901),
        ("S1901M", 1901),
        ("FALL 1902 RETREAT", 1902),
        ("F1902R", 1902),
        ("WINTER 1903 ADJUSTMENTS", 1903),
        ("W1903A", 1903),
        ("FORMING", None),
        ("COMPLETED", None),
        ("S1901MVT", 1901),
        ("AUTUMN 1901 BUILD", 1901),
        ("A1901B", 1901),
        ("Random String", None),
        ("S19ABM", None), # Non-digit year
        ("S190M", None), # Too short year
        ("S2023M", 2023), # Modern year
        ("Spring 2023 Movement", 2023),
        ("", None),
        (None, None),
    ],
)
def test_extract_year_from_phase(phase_string, expected_year):
    assert _extract_year_from_phase(phase_string) == expected_year 



================================================
File: unit/orchestrators/__init__.py
================================================



================================================
File: unit/orchestrators/test_build_strategy.py
================================================
import pytest
import asyncio
from unittest.mock import MagicMock, AsyncMock
from types import SimpleNamespace

from ai_diplomacy.orchestrators.build import BuildPhaseStrategy


class FakeGame:
    def __init__(self, phase, powers_names, build_conditions=None):
        self.phase = phase
        self.year = int(phase[1:5]) if phase and len(phase) >= 5 and phase[1:5].isdigit() else 1901
        self.powers = {}
        for name in powers_names:
            n_builds_val = build_conditions.get(name, 0) if build_conditions else 0
            self.powers[name] = SimpleNamespace(
                is_eliminated=lambda: False,
                must_retreat=False, # Not relevant for build
                n_builds=n_builds_val
            )
    def get_current_phase(self):
        return self.phase
    def get_state(self):
        return {"centers": {}}

class DummyOrchestrator:
    def __init__(self, active_powers_list, game_config_mock, agent_manager_mock):
        self.active_powers = active_powers_list
        self.config = game_config_mock 
        self.agent_manager = agent_manager_mock
        self._get_orders_for_power = AsyncMock(return_value=["A PAR B"])
        self.get_valid_orders_func = None 

@pytest.mark.asyncio
async def test_build_generates_orders_for_building_power(mocker):
    strat = BuildPhaseStrategy()
    powers = ["ENG", "FRA"]
    # FRA has 1 build, ENG has 0
    build_conditions = {"FRA": 1, "ENG": 0}
    fake_game = FakeGame("W1901B", powers, build_conditions)
    
    mock_game_config = MagicMock()
    mock_agent_manager = MagicMock()
    mock_agent_fra = MagicMock()
    mock_agent_eng = MagicMock()

    def get_agent_side_effect(power_name):
        if power_name == "FRA": return mock_agent_fra
        if power_name == "ENG": return mock_agent_eng
        return MagicMock()
    mock_agent_manager.get_agent.side_effect = get_agent_side_effect

    dummy_orchestrator = DummyOrchestrator(powers, mock_game_config, mock_agent_manager)
    dummy_orchestrator._get_orders_for_power = AsyncMock(return_value=["A PAR B"])
    
    mock_game_history = MagicMock()
    mock_game_history.add_orders = MagicMock()

    orders = await strat.get_orders(fake_game, dummy_orchestrator, mock_game_history)

    dummy_orchestrator._get_orders_for_power.assert_awaited_once_with(
        fake_game, "FRA", mock_agent_fra, mock_game_history
    )
    mock_game_history.add_orders.assert_called_once_with(fake_game.get_current_phase(), "FRA", ["A PAR B"])
    
    assert isinstance(orders, dict)
    assert set(orders.keys()) == set(powers)
    assert orders["FRA"] == ["A PAR B"]
    assert orders["ENG"] == []

@pytest.mark.asyncio
async def test_build_generates_orders_for_disbanding_power(mocker):
    strat = BuildPhaseStrategy()
    powers = ["GER"]
    # GER has -1 builds (must disband)
    build_conditions = {"GER": -1}
    fake_game = FakeGame("W1901B", powers, build_conditions)
    
    mock_game_config = MagicMock()
    mock_agent_manager = MagicMock()
    mock_agent_ger = MagicMock()
    mock_agent_manager.get_agent.return_value = mock_agent_ger

    dummy_orchestrator = DummyOrchestrator(powers, mock_game_config, mock_agent_manager)
    dummy_orchestrator._get_orders_for_power = AsyncMock(return_value=["A BER D"])
    
    mock_game_history = MagicMock()
    mock_game_history.add_orders = MagicMock()

    orders = await strat.get_orders(fake_game, dummy_orchestrator, mock_game_history)

    dummy_orchestrator._get_orders_for_power.assert_awaited_once_with(
        fake_game, "GER", mock_agent_ger, mock_game_history
    )
    mock_game_history.add_orders.assert_called_once_with(fake_game.get_current_phase(), "GER", ["A BER D"])
    
    assert isinstance(orders, dict)
    assert orders["GER"] == ["A BER D"]

@pytest.mark.asyncio
async def test_build_no_building_or_disbanding_powers(mocker):
    strat = BuildPhaseStrategy()
    powers = ["ENG", "GER"]
    build_conditions = {"ENG": 0, "GER": 0} # No builds or disbands
    fake_game = FakeGame("W1901B", powers, build_conditions)
    
    mock_game_config = MagicMock()
    mock_agent_manager = MagicMock()
    dummy_orchestrator = DummyOrchestrator(powers, mock_game_config, mock_agent_manager)
    mock_game_history = MagicMock()
    mock_game_history.add_orders = MagicMock()

    orders = await strat.get_orders(fake_game, dummy_orchestrator, mock_game_history)

    dummy_orchestrator._get_orders_for_power.assert_not_awaited()
    mock_game_history.add_orders.assert_not_called()
    assert orders == {} 

@pytest.mark.asyncio
async def test_build_agent_fails_to_provide_orders(mocker):
    strat = BuildPhaseStrategy()
    powers = ["ITA"]
    build_conditions = {"ITA": 1} # ITA has one build
    fake_game = FakeGame("W1901B", powers, build_conditions)
    
    mock_game_config = MagicMock()
    mock_agent_manager = MagicMock()
    mock_agent_ita = MagicMock()
    mock_agent_manager.get_agent.return_value = mock_agent_ita

    dummy_orchestrator = DummyOrchestrator(powers, mock_game_config, mock_agent_manager)
    dummy_orchestrator._get_orders_for_power = AsyncMock(side_effect=Exception("LLM error"))
    
    mock_game_history = MagicMock()
    mock_game_history.add_orders = MagicMock()

    orders = await strat.get_orders(fake_game, dummy_orchestrator, mock_game_history)

    dummy_orchestrator._get_orders_for_power.assert_awaited_once_with(
        fake_game, "ITA", mock_agent_ita, mock_game_history
    )
    mock_game_history.add_orders.assert_called_once_with(fake_game.get_current_phase(), "ITA", [])
    
    assert isinstance(orders, dict)
    assert orders["ITA"] == [] 


================================================
File: unit/orchestrators/test_movement_strategy.py
================================================
import pytest
import asyncio
from unittest.mock import MagicMock, AsyncMock
from types import SimpleNamespace # For FakeGame powers

from ai_diplomacy.orchestrators.movement import MovementPhaseStrategy
# Assuming PhaseOrchestrator, GameHistory, AgentManager, GameConfig might be needed for DummyOrchestrator
# We'll mock or simplify them as much as possible.
# For GameConfig, we only need a few attributes like powers_and_models if used by active_powers logic
# For AgentManager, we need get_agent method.

# Simplified FakeGame as per the plan
class FakeGame:
    def __init__(self, phase, powers_names):
        self.phase = phase
        self.year = int(phase[1:5]) if phase and len(phase) >= 5 and phase[1:5].isdigit() else 1901
        self.powers = {
            name: SimpleNamespace(
                is_eliminated=lambda: False,
                must_retreat=False, 
                n_builds=1 # Default, not relevant for movement
            )
            for name in powers_names
        }
    def get_current_phase(self):
        return self.phase
    def get_state(self): # Required by _get_orders_for_power if not LLMAgent
        return {"centers": {}}

# Dummy Orchestrator to hold necessary attributes/methods for the strategy
class DummyOrchestrator:
    def __init__(self, active_powers_list, game_config_mock, agent_manager_mock, get_valid_orders_func_mock=None):
        self.active_powers = active_powers_list
        self.config = game_config_mock # Needs llm_log_path if get_valid_orders_func is used
        self.agent_manager = agent_manager_mock # Needs get_agent
        # _get_orders_for_power is called by the strategy, so we mock it here.
        # Alternatively, the strategy itself could be more self-contained or take more direct dependencies.
        self._get_orders_for_power = AsyncMock(return_value=["WAIVE"])
        # get_valid_orders_func might be needed if _get_orders_for_power calls it for non-LLMAgents
        self.get_valid_orders_func = get_valid_orders_func_mock 

@pytest.mark.asyncio
async def test_movement_generates_orders(mocker):
    strat = MovementPhaseStrategy()
    powers = ["ENG", "FRA"]
    fake_game = FakeGame("S1901M", powers)
    
    # Mock dependencies for DummyOrchestrator
    mock_game_config = MagicMock()
    # mock_game_config.llm_log_path = "/tmp/dummy_log_path" # If using get_valid_orders_func
    mock_game_config.num_negotiation_rounds = 1 # For perform_negotiation_rounds

    mock_agent_manager = MagicMock()
    # Simplistic agent mock for _get_orders_for_power to function if it checks agent type
    mock_agent = MagicMock()
    mock_agent_manager.get_agent.return_value = mock_agent

    dummy_orchestrator = DummyOrchestrator(powers, mock_game_config, mock_agent_manager)

    # Mock GameHistory (passed to strategy's get_orders)
    mock_game_history = MagicMock()
    mock_game_history.add_orders = MagicMock() # Called by the strategy
    mock_game_history.add_phase = MagicMock() # Called by perform_negotiation_rounds
    mock_game_history.add_message = MagicMock() # Called by perform_negotiation_rounds

    # Patch external calls made by MovementPhaseStrategy or its helpers
    # perform_negotiation_rounds is now a separate function imported by movement.py
    mocked_perform_negotiation = mocker.patch(
        "ai_diplomacy.orchestrators.movement.perform_negotiation_rounds", 
        new_callable=AsyncMock, 
        return_value=None
    )
    # _get_orders_for_power is a method on the orchestrator, already mocked in DummyOrchestrator
    # gather_possible_orders is used by orchestrator._get_orders_for_power for non-LLM agents
    # For this test, _get_orders_for_power on DummyOrchestrator is an AsyncMock, so gather_possible_orders won't be hit
    # unless we were testing the non-LLM path of _get_orders_for_power itself.
    # mocker.patch("ai_diplomacy.utils.gather_possible_orders", return_value={"A LON": ["A LON H"]})

    orders = await strat.get_orders(fake_game, dummy_orchestrator, mock_game_history)

    # Assertions
    mocked_perform_negotiation.assert_awaited_once_with(
        fake_game, mock_game_history, dummy_orchestrator.agent_manager, 
        dummy_orchestrator.active_powers, dummy_orchestrator.config
    )
    
    # Check that _get_orders_for_power was called for each active power
    assert dummy_orchestrator._get_orders_for_power.await_count == len(powers)
    for power_name in powers:
        # Check if _get_orders_for_power was called with this power_name
        # This is a bit tricky as it's called multiple times. 
        # We can check call_args_list
        power_found_in_calls = False
        for call in dummy_orchestrator._get_orders_for_power.call_args_list:
            args, _ = call
            if args[1] == power_name: # game, power_name, agent, game_history
                power_found_in_calls = True
                break
        assert power_found_in_calls, f"_get_orders_for_power not called for {power_name}"

    # Check that game_history.add_orders was called for each power
    assert mock_game_history.add_orders.call_count == len(powers)
    for power_name in powers:
        # Check if add_orders was called correctly
        # Example: mock_game_history.add_orders.assert_any_call(fake_game.get_current_phase(), power_name, ["WAIVE"])
        # This depends on the exact orders returned by the mocked _get_orders_for_power
        # Since _get_orders_for_power is mocked on DummyOrchestrator to return ["WAIVE"], 
        # and this return is used by the strategy directly.
        power_order_added = False
        for call in mock_game_history.add_orders.call_args_list:
            args, _ = call
            if args[0] == fake_game.get_current_phase() and args[1] == power_name and args[2] == ["WAIVE"]:
                power_order_added = True
                break
        assert power_order_added, f"game_history.add_orders not called correctly for {power_name}"

    # Validate the structure of returned orders
    assert isinstance(orders, dict)
    assert set(orders.keys()) == set(powers)
    for power_name in powers:
        assert orders[power_name] == ["WAIVE"] # Based on DummyOrchestrator's _get_orders_for_power mock




================================================
File: unit/orchestrators/test_retreat_strategy.py
================================================
import pytest
import asyncio
from unittest.mock import MagicMock, AsyncMock
from types import SimpleNamespace

from ai_diplomacy.orchestrators.retreat import RetreatPhaseStrategy

# Using the same FakeGame and DummyOrchestrator definitions as in test_movement_strategy
# Ideally, these would be in a shared conftest.py or a common test utility file.
# For now, duplicating for simplicity until a broader test structure is decided.

class FakeGame:
    def __init__(self, phase, powers_names, retreat_conditions=None):
        self.phase = phase
        self.year = int(phase[1:5]) if phase and len(phase) >= 5 and phase[1:5].isdigit() else 1901
        self.powers = {}
        for name in powers_names:
            must_retreat_val = retreat_conditions.get(name, False) if retreat_conditions else False
            self.powers[name] = SimpleNamespace(
                is_eliminated=lambda: False,
                must_retreat=must_retreat_val,
                n_builds=0 
            )
    def get_current_phase(self):
        return self.phase
    def get_state(self):
        return {"centers": {}}

class DummyOrchestrator:
    def __init__(self, active_powers_list, game_config_mock, agent_manager_mock):
        self.active_powers = active_powers_list
        self.config = game_config_mock 
        self.agent_manager = agent_manager_mock
        self._get_orders_for_power = AsyncMock(return_value=["A BUD - STP"])
        self.get_valid_orders_func = None 

@pytest.mark.asyncio
async def test_retreat_generates_orders_for_retreating_power(mocker):
    strat = RetreatPhaseStrategy()
    powers = ["ENG", "FRA"]
    retreat_conditions = {"FRA": True, "ENG": False}
    fake_game = FakeGame("F1901R", powers, retreat_conditions)
    
    mock_game_config = MagicMock()
    mock_agent_manager = MagicMock()
    mock_agent_fra = MagicMock()
    mock_agent_eng = MagicMock()

    def get_agent_side_effect(power_name):
        if power_name == "FRA": return mock_agent_fra
        if power_name == "ENG": return mock_agent_eng
        return MagicMock()
    mock_agent_manager.get_agent.side_effect = get_agent_side_effect

    dummy_orchestrator = DummyOrchestrator(powers, mock_game_config, mock_agent_manager)
    dummy_orchestrator._get_orders_for_power = AsyncMock(return_value=["A PAR R A MAR"])
    
    mock_game_history = MagicMock()
    mock_game_history.add_orders = MagicMock()

    orders = await strat.get_orders(fake_game, dummy_orchestrator, mock_game_history)

    dummy_orchestrator._get_orders_for_power.assert_awaited_once_with(
        fake_game, "FRA", mock_agent_fra, mock_game_history
    )
    mock_game_history.add_orders.assert_called_once_with(fake_game.get_current_phase(), "FRA", ["A PAR R A MAR"])
    
    assert isinstance(orders, dict)
    assert set(orders.keys()) == set(powers)
    assert orders["FRA"] == ["A PAR R A MAR"]
    assert orders["ENG"] == []

@pytest.mark.asyncio
async def test_retreat_no_retreating_powers(mocker):
    strat = RetreatPhaseStrategy()
    powers = ["ENG", "GER"]
    retreat_conditions = {"ENG": False, "GER": False}
    fake_game = FakeGame("F1901R", powers, retreat_conditions)
    
    mock_game_config = MagicMock()
    mock_agent_manager = MagicMock()
    dummy_orchestrator = DummyOrchestrator(powers, mock_game_config, mock_agent_manager)
    mock_game_history = MagicMock()
    mock_game_history.add_orders = MagicMock()

    orders = await strat.get_orders(fake_game, dummy_orchestrator, mock_game_history)

    dummy_orchestrator._get_orders_for_power.assert_not_awaited()
    mock_game_history.add_orders.assert_not_called()
    assert orders == {}

@pytest.mark.asyncio
async def test_retreat_agent_fails_to_provide_orders(mocker):
    strat = RetreatPhaseStrategy()
    powers = ["ITA"]
    retreat_conditions = {"ITA": True}
    fake_game = FakeGame("F1901R", powers, retreat_conditions)
    
    mock_game_config = MagicMock()
    mock_agent_manager = MagicMock()
    mock_agent_ita = MagicMock()
    mock_agent_manager.get_agent.return_value = mock_agent_ita

    dummy_orchestrator = DummyOrchestrator(powers, mock_game_config, mock_agent_manager)
    dummy_orchestrator._get_orders_for_power = AsyncMock(side_effect=Exception("LLM error"))
    
    mock_game_history = MagicMock()
    mock_game_history.add_orders = MagicMock()

    orders = await strat.get_orders(fake_game, dummy_orchestrator, mock_game_history)

    dummy_orchestrator._get_orders_for_power.assert_awaited_once_with(
        fake_game, "ITA", mock_agent_ita, mock_game_history
    )
    mock_game_history.add_orders.assert_called_once_with(fake_game.get_current_phase(), "ITA", [])
    
    assert isinstance(orders, dict)
    assert orders["ITA"] == []
 


